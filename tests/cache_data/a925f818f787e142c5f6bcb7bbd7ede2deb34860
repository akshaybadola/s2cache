{"details": {"paperId": "a925f818f787e142c5f6bcb7bbd7ede2deb34860", "title": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations", "citationCount": 285, "influentialCitationCount": 26, "authors": [{"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}, {"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}], "abstract": "By design, word embeddings are unable to model the dynamic nature of words\u2019 semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2018, "url": "https://www.semanticscholar.org/paper/a925f818f787e142c5f6bcb7bbd7ede2deb34860", "externalIds": {"ACL": "N19-1128", "MAG": "2927103915", "DBLP": "conf/naacl/PilehvarC19", "ArXiv": "1808.09121", "DOI": "10.18653/v1/N19-1128", "CorpusId": 102353817}, "citations": [], "references": []}, "citations": {"offset": 0, "data": [{"contexts": [], "citingPaper": {"paperId": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4", "externalIds": {"DBLP": "journals/corr/abs-2308-05342", "ArXiv": "2308.05342", "DOI": "10.48550/arXiv.2308.05342", "CorpusId": 260775822}, "url": "https://www.semanticscholar.org/paper/896ca0a68e4d33d76a7366bcab85eb7d2605a8c4", "title": "Metacognitive Prompting Improves Understanding in Large Language Models", "abstract": "In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting methods, including standard and chain-of-thought prompting. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2108035197", "name": "Yuqing Wang"}, {"authorId": "47827165", "name": "Yun Zhao"}]}}, {"contexts": [], "citingPaper": {"paperId": "62487b15a1375f896b53195c43cea7c52976c361", "externalIds": {"DBLP": "journals/corr/abs-2308-03582", "ArXiv": "2308.03582", "DOI": "10.48550/arXiv.2308.03582", "CorpusId": 260680766}, "url": "https://www.semanticscholar.org/paper/62487b15a1375f896b53195c43cea7c52976c361", "title": "WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset", "abstract": "A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to 'learn' new information. While model-centric solutions like continual learning or parameter-efficient fine tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resource can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic, and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, showing promising results with respect to competitive baselines.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2022477509", "name": "Hsuvas Borkakoty"}, {"authorId": "2254466", "name": "Luis Espinosa Anke"}]}}, {"contexts": ["Camacho-Collados, 2019) or specificity-controlled glossary writing."], "citingPaper": {"paperId": "0aa7d8f65425d194c576f09c76ceb7d78e594e96", "externalIds": {"ArXiv": "2308.03043", "DBLP": "journals/corr/abs-2308-03043", "DOI": "10.48550/arXiv.2308.03043", "CorpusId": 260681823}, "url": "https://www.semanticscholar.org/paper/0aa7d8f65425d194c576f09c76ceb7d78e594e96", "title": "3D-EX : A Unified Dataset of Definitions and Dictionary Examples", "abstract": "Definitions are a fundamental building block in lexicography, linguistics and computational semantics. In NLP, they have been used for retrofitting word embeddings or augmenting contextual representations in language models. However, lexical resources containing definitions exhibit a wide range of properties, which has implications in the behaviour of models trained and evaluated on them. In this paper, we introduce 3D- EX , a dataset that aims to fill this gap by combining well-known English resources into one centralized knowledge repository in the form oftriples. 3D- EX is a unified evaluation framework with carefully pre-computed train/validation/test splits to prevent memorization. We report experimental results that suggest that this dataset could be effectively leveraged in downstream NLP tasks. Code and data are available at https://github.com/F-Almeman/3D-EX .", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "6439177", "name": "F. Almeman"}, {"authorId": "2230095007", "name": "Hadi Sheikhi"}, {"authorId": "2254466", "name": "Luis Espinosa Anke"}]}}, {"contexts": ["Furthermore, Pruksachatkun et al. (2020) observed that STILT is particularly effective in target tasks in NLU with smaller datasets, e.g. WiC (Pilehvar and Camacho-Collados, 2019) and BoolQ (Clark et al., 2019)."], "citingPaper": {"paperId": "8e92c308de38e2958ce3b974cc736cd361e31c5a", "externalIds": {"DBLP": "journals/corr/abs-2308-00528", "ArXiv": "2308.00528", "DOI": "10.48550/arXiv.2308.00528", "CorpusId": 260350945}, "url": "https://www.semanticscholar.org/paper/8e92c308de38e2958ce3b974cc736cd361e31c5a", "title": "Unimodal Intermediate Training for Multimodal Meme Sentiment Classification", "abstract": "Internet Memes remain a challenging form of user-generated content for automated sentiment classification. The availability of labelled memes is a barrier to developing sentiment classifiers of multimodal memes. To address the shortage of labelled memes, we propose to supplement the training of a multimodal meme classifier with unimodal (image-only and text-only) data. In this work, we present a novel variant of supervised intermediate training that uses relatively abundant sentiment-labelled unimodal data. Our results show a statistically significant performance improvement from the incorporation of unimodal text data. Furthermore, we show that the training set of labelled memes can be reduced by 40% without reducing the performance of the downstream model.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2210392249", "name": "Muzhaffar Hazman"}, {"authorId": "143667097", "name": "Susan Mckeever"}, {"authorId": "34832259", "name": "J. Griffith"}]}}, {"contexts": ["We evaluate CPET on 11 datasets, covering typical NLP tasks, including BoolQ (Clark et al., 2019), CB (De Marneffe et al., 2019), RTE (Bentivogli et al., 2009; Wang et al., 2019), COPA (Roemmele et al., 2011), WiC (Pilehvar and CamachoCollados, 2018), SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QQP (Wang et al., 2018), MNLI (Williams et al., 2017), QNLI (Rajpurkar et al., 2016), and SQuAD (Rajpurkar et al., 2016).", ", 2011), WiC (Pilehvar and CamachoCollados, 2018), SST-2 (Socher et al."], "citingPaper": {"paperId": "291955cc13853de19c58337d6a68816d00c2fa74", "externalIds": {"DBLP": "journals/corr/abs-2307-07705", "ArXiv": "2307.07705", "DOI": "10.48550/arXiv.2307.07705", "CorpusId": 259937262}, "url": "https://www.semanticscholar.org/paper/291955cc13853de19c58337d6a68816d00c2fa74", "title": "CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models", "abstract": "Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named\"CPET\". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating task-specific PET modules with a compressed LLM can achieve comparable performance to collaborating PET modules with the original version of the compressed LLM and outperform directly applying vanilla PET methods to the compressed LLM.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2150606888", "name": "Weilin Zhao"}, {"authorId": "2214586078", "name": "Yuxiang Huang"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "2141313179", "name": "Zhiyuan Liu"}, {"authorId": "2621696", "name": "Zhengyan Zhang"}, {"authorId": "1753344", "name": "Maosong Sun"}]}}, {"contexts": ["We use the SuperGLUE dataset collection to evaluate model performance, specifically focusing on RTE (Dagan et al., 2005), BoolQ (Clark et al., 2019), WSC (Levesque et al., 2012), WIC (Pilehvar & Camacho-Collados, 2019), MultiRC (Khashabi et al., 2018), and COPA (Roemmele et al., 2011)."], "citingPaper": {"paperId": "f4bdec0cf595720bc8ee5df2196324bac8f52ab4", "externalIds": {"DBLP": "journals/corr/abs-2306-09782", "ArXiv": "2306.09782", "DOI": "10.48550/arXiv.2306.09782", "CorpusId": 259187846}, "url": "https://www.semanticscholar.org/paper/f4bdec0cf595720bc8ee5df2196324bac8f52ab4", "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.", "venue": "arXiv.org", "year": 2023, "citationCount": 5, "influentialCitationCount": 0, "authors": [{"authorId": "2055634356", "name": "Kai Lv"}, {"authorId": "2145435513", "name": "Yuqing Yang"}, {"authorId": "2136108329", "name": "Tengxiao Liu"}, {"authorId": "2188485375", "name": "Qi-jie Gao"}, {"authorId": "153683057", "name": "Qipeng Guo"}, {"authorId": "2188058565", "name": "Xipeng Qiu"}]}}, {"contexts": ["We follow Liu et al. [41] and use T0-3B [62] as the base model and finetune (IA)3 models on the train split of eleven datasets including sentence completion (COPA [57], H-SWAG [80], and Story Cloze [64] datasets), natural language inference (ANLI [47], CB [42], and RTE [11]), coreference resolution (WSC [37] and Winogrande [60]), and word sense disambiguation (WiC [49]).", "ANLI [47], WiC [49], WSC [37], and Story Cloze [64], QuaRTz [68], Cars [35], GTSRB [67] are under Creative Commons License.", "[41] and use T0-3B [62] as the base model and finetune (IA)(3) models on the train split of eleven datasets including sentence completion (COPA [57], H-SWAG [80], and Story Cloze [64] datasets), natural language inference (ANLI [47], CB [42], and RTE [11]), coreference resolution (WSC [37] and Winogrande [60]), and word sense disambiguation (WiC [49]).", "Specifically, we report the average performance over the following tasks and datasets: Cosmos QA [27], Social IQA [63], and QuAIL [58] for question answering; WiC [49] for word sense disambiguation; and COPA [57], and H-SWAG [80] for sentence completion."], "citingPaper": {"paperId": "29bfc8703709c437d16c478c50ae599312ae3036", "externalIds": {"DBLP": "journals/corr/abs-2306-01708", "ArXiv": "2306.01708", "DOI": "10.48550/arXiv.2306.01708", "CorpusId": 259064039}, "url": "https://www.semanticscholar.org/paper/29bfc8703709c437d16c478c50ae599312ae3036", "title": "Resolving Interference When Merging Models", "abstract": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TrIm, Elect Sign&Merge (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "46841632", "name": "Prateek Yadav"}, {"authorId": "1390031652", "name": "Derek Tam"}, {"authorId": "41019330", "name": "Leshem Choshen"}, {"authorId": "2402716", "name": "Colin Raffel"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}}, {"contexts": ["Pina and Johansson (2014) address the varying polysemy problem of sense representation by setting the number of senses of a word as defined by a sense inventory.", "\u2026the issues brought by a two-step clustering, the idea of clustering context vectors has been adapted into the training of word embeddings (Tian et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2014; Liu et al., 2015b,a; Bartunov et al., 2016; Lee and Chen, 2017; Nguyen et al., 2017)."], "citingPaper": {"paperId": "24ab514af7c9ffea6d34ca7c52763bb501a0d38a", "externalIds": {"DBLP": "journals/corr/abs-2306-01457", "ACL": "2023.trustnlp-1.2", "ArXiv": "2306.01457", "DOI": "10.48550/arXiv.2306.01457", "CorpusId": 259064136}, "url": "https://www.semanticscholar.org/paper/24ab514af7c9ffea6d34ca7c52763bb501a0d38a", "title": "Driving Context into Text-to-Text Privatization", "abstract": "Metric Differential Privacy enables text-to-text privatization by adding calibrated noise to the vector of a word derived from an embedding space and projecting this noisy vector back to a discrete vocabulary using a nearest neighbor search. Since words are substituted without context, this mechanism is expected to fall short at finding substitutes for words with ambiguous meanings, such as \u2018bank\u2019. To account for these ambiguous words, we leverage a sense embedding and incorporate a sense disambiguation step prior to noise injection. We encompass our modification to the privatization mechanism with an estimation of privacy and utility. For word sense disambiguation on the Words in Context dataset, we demonstrate a substantial increase in classification accuracy by 6.05%.", "venue": "TRUSTNLP", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2053374090", "name": "Stefan Arnold"}, {"authorId": "2056769699", "name": "Dilara Yesilbas"}, {"authorId": "113190661", "name": "Sven Weinzierl"}]}}, {"contexts": [], "citingPaper": {"paperId": "83e49a574951789b4cb08d0897fe275ddb8c1553", "externalIds": {"DBLP": "journals/corr/abs-2305-19585", "ACL": "2023.acl-long.571", "ArXiv": "2305.19585", "DOI": "10.48550/arXiv.2305.19585", "CorpusId": 258987563}, "url": "https://www.semanticscholar.org/paper/83e49a574951789b4cb08d0897fe275ddb8c1553", "title": "LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction", "abstract": "Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until later encoding stages.To this end, we introduce Layer-Adjustable Interactions in Transformers (LAIT). Within LAIT, segmented inputs are first encoded independently, and then jointly. This partial two-tower architecture bridges the gap between a Dual Encoder\u2019s ability to pre-compute representations for segments and a fully self-attentive Transformer\u2019s capacity to model cross-segment attention. The LAIT framework effectively leverages existing pretrained Transformers and converts them into the hybrid of the two aforementioned architectures, allowing for easy and intuitive control over the performance-efficiency tradeoff. Experimenting on a wide range of NLP tasks, we find LAIT able to reduce 30-50% of the attention FLOPs on many tasks, while preserving high accuracy; in some practical settings, LAIT could reduce actual latency by orders of magnitude.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "authors": [{"authorId": "2139750731", "name": "Jeremiah Milbauer"}, {"authorId": "1767336", "name": "Annie Louis"}, {"authorId": "2029657350", "name": "Mohammad Javad Hosseini"}, {"authorId": "1983135", "name": "Alex Fabrikant"}, {"authorId": "1680617", "name": "Donald Metzler"}, {"authorId": "32303439", "name": "Tal Schuster"}]}}, {"contexts": ["Prior work has shown that sense embeddings are useful for tasks such as Word Sense Disambiguation (WSD) and sense discrimination tasks such as Word in Context (WiC) (Loureiro and Jorge, 2019b; Pilehvar and Camacho-Collados, 2019)."], "citingPaper": {"paperId": "e44905c4af9f01cee3c2b3d2ddf1d40338bf2905", "externalIds": {"DBLP": "journals/corr/abs-2305-19092", "ArXiv": "2305.19092", "DOI": "10.48550/arXiv.2305.19092", "CorpusId": 258967290}, "url": "https://www.semanticscholar.org/paper/e44905c4af9f01cee3c2b3d2ddf1d40338bf2905", "title": "Together We Make Sense - Learning Meta-Sense Embeddings from Pretrained Static Sense Embeddings", "abstract": "Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses. For this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning methods. However, not all existing sense embeddings cover all senses of ambiguous words equally well due to the discrepancies in their training resources. To address this problem, we propose the first-ever meta-sense embedding method -- Neighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings by combining multiple independently trained source sense embeddings such that the sense neighbourhoods computed from the source embeddings are preserved in the meta-embedding space. Our proposed method can combine source sense embeddings that cover different sets of word senses. Experimental results on Word Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the proposed meta-sense embedding method consistently outperforms several competitive baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "30152292", "name": "Haochen Luo"}, {"authorId": "32066669", "name": "Yi Zhou"}, {"authorId": "2075356592", "name": "D. Bollegala"}]}}, {"contexts": ["RTE 2006; 2006; 2007; 2009 Dev 278 ChatGPT Dec 15 Version Human WiC (Pilehvar and Camacho-Collados, 2019) Dev 638 ChatGPT Dec 15 Version Human WSC (Levesque et al."], "citingPaper": {"paperId": "d3060876d9ad4e4e50e1c88a8c04186df00f24e2", "externalIds": {"DBLP": "conf/acl/LaskarBRBJH23", "ArXiv": "2305.18486", "DOI": "10.48550/arXiv.2305.18486", "CorpusId": 258967462}, "url": "https://www.semanticscholar.org/paper/d3060876d9ad4e4e50e1c88a8c04186df00f24e2", "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets", "abstract": "The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 10, "influentialCitationCount": 0, "authors": [{"authorId": "46437970", "name": "Md Tahmid Rahman Laskar"}, {"authorId": "31773000", "name": "M Saiful Bari"}, {"authorId": "2218664824", "name": "Mizanur Rahman"}, {"authorId": "145505476", "name": "Md Amran Hossen Bhuiyan"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "1683391", "name": "J. Huang"}]}}, {"contexts": ["Turton et al. (2020) evaluate the Binder feature prediction model using the Words in Context Dataset (Pilehvar and Camacho-Collados, 2019), which only labels token pairs as \u2018same meaning\u2019 or \u2018different meaning\u2019.", "(2020) evaluate the Binder feature prediction model using the Words in Context Dataset (Pilehvar and Camacho-Collados, 2019), which only labels token pairs as \u2018same meaning\u2019 or \u2018different meaning\u2019."], "citingPaper": {"paperId": "4b260998bf4384c223efb852791fac096860b24e", "externalIds": {"DBLP": "journals/corr/abs-2305-18598", "ArXiv": "2305.18598", "ACL": "2023.acl-long.14", "DOI": "10.48550/arXiv.2305.18598", "CorpusId": 258967700}, "url": "https://www.semanticscholar.org/paper/4b260998bf4384c223efb852791fac096860b24e", "title": "A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces", "abstract": "We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., \u2018a beautiful three days\u2019). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2008204336", "name": "Gabriella Chronis"}, {"authorId": "2412497", "name": "Kyle Mahowald"}, {"authorId": "1708114", "name": "K. Erk"}]}}, {"contexts": ["[74] Mohammad Taher Pilehvar and Jose Camacho-Collados.", "For OPT experiments, we consider the SuperGLUE dataset collection [92], including: BoolQ [20], CB [23], COPA [78], MultiRC [49], ReCoRD [104], RTE [21, 8, 35, 9], WiC [74], and WSC [53]."], "citingPaper": {"paperId": "95f4426036fb08ace9ea4054a5a4d76119e5fcf7", "externalIds": {"ArXiv": "2305.17333", "DBLP": "journals/corr/abs-2305-17333", "DOI": "10.48550/arXiv.2305.17333", "CorpusId": 258959274}, "url": "https://www.semanticscholar.org/paper/95f4426036fb08ace9ea4054a5a4d76119e5fcf7", "title": "Fine-Tuning Language Models with Just Forward Passes", "abstract": "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.", "venue": "arXiv.org", "year": 2023, "citationCount": 7, "influentialCitationCount": 1, "authors": [{"authorId": "49288855", "name": "Sadhika Malladi"}, {"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "19209293", "name": "Eshaan Nichani"}, {"authorId": "31645393", "name": "Alexandru Damian"}, {"authorId": "2108327687", "name": "Jason D. Lee"}, {"authorId": "50536468", "name": "Danqi Chen"}, {"authorId": "145563465", "name": "Sanjeev Arora"}]}}, {"contexts": ["\u2022 WiC (Pilehvar and Camacho-Collados, 2019): WiC has the same input format as MNLI and RTE."], "citingPaper": {"paperId": "4e16bfc8ded08fbec67666869f39c043a6770946", "externalIds": {"DBLP": "conf/acl/LiaoMM23", "ACL": "2023.acl-long.233", "ArXiv": "2305.16742", "DOI": "10.48550/arXiv.2305.16742", "CorpusId": 258947572}, "url": "https://www.semanticscholar.org/paper/4e16bfc8ded08fbec67666869f39c043a6770946", "title": "Parameter-Efficient Fine-Tuning without Introducing New Latency", "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full fine-tuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03% parameters of full fine-tuning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "66693547", "name": "Baohao Liao"}, {"authorId": "2218732291", "name": "Yan Meng"}, {"authorId": "2062908179", "name": "C. Monz"}]}}, {"contexts": ["We start with SAE sentences sampled from the Word-in-Context (WiC) Dataset (Pilehvar and Camacho-Collados, 2019)."], "citingPaper": {"paperId": "61d40d7263d9c03649fcd6ccb840ef8dac2243d1", "externalIds": {"DBLP": "conf/acl/HeldZY23", "ArXiv": "2305.16651", "DOI": "10.48550/arXiv.2305.16651", "CorpusId": 258947682}, "url": "https://www.semanticscholar.org/paper/61d40d7263d9c03649fcd6ccb840ef8dac2243d1", "title": "TADA: Task-Agnostic Dialect Adapters for English", "abstract": "Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task-specific data or synthetic data augmentation, both of which require intervention for each dialect and task pair. This poses a scalability issue that prevents the broad adoption of robust dialectal English NLP. We introduce a simple yet effective method for task-agnostic dialect adaptation by aligning non-SAE dialects using adapters and composing them with task-specific adapters from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on 4 dialectal variants of the GLUE benchmark without task-specific supervision.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "46552910", "name": "William B. Held"}, {"authorId": "1399135100", "name": "Caleb Ziems"}, {"authorId": "2143919864", "name": "Diyi Yang"}]}}, {"contexts": ["We use datasets involving SuperGLUE (Wang et al., 2019) benchmark including BoolQ (Clark et al., 2019a), COPA (Roemmele et al., 2011), RTE (Wang et al., 2018), WiC (Pilehvar and Camacho-Collados, 2019) and WSC (Levesque et al., 2012) as well as 3 Named Entity Recognition (NER) tasks including CoNLL03 (Tjong Kim Sang and De Meulder, 2003), CoNLL04 (Carreras and M\u00e0rquez, 2004), and OntoNotes 5.0 (Weischedel et al., 2013).", ", 2018), WiC (Pilehvar and Camacho-Collados, 2019) and WSC (Levesque et al.", "We conduct 5 NLU tasks on SuperGLUE (Wang et al., 2019) benchmark including BoolQ (Clark et al., 2019a), COPA (Roemmele et al., 2011), RTE (Wang et al., 2018), WiC (Pilehvar and Camacho-Collados, 2019) and WSC (Levesque et al., 2012) as well as 3 Named Entity Recognition (NER) tasks including CoNLL03 (Tjong Kim Sang and De Meulder, 2003), CoNLL04 (Carreras and M\u00e0rquez, 2004), and OntoNotes 5.0 (Weischedel et al., 2013)."], "citingPaper": {"paperId": "5193003d574eff310742e6ce94612fc82851fee0", "externalIds": {"DBLP": "conf/acl/ZhangTX00H23", "ArXiv": "2305.15212", "ACL": "2023.acl-short.107", "DOI": "10.48550/arXiv.2305.15212", "CorpusId": 258865689}, "url": "https://www.semanticscholar.org/paper/5193003d574eff310742e6ce94612fc82851fee0", "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning", "abstract": "Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2116702333", "name": "Zhenru Zhang"}, {"authorId": "2111727840", "name": "Chuanqi Tan"}, {"authorId": "153194420", "name": "Haiyang Xu"}, {"authorId": "121899912", "name": "Chengyu Wang"}, {"authorId": "2078113", "name": "Jun Huang"}, {"authorId": "2410938", "name": "Songfang Huang"}]}}, {"contexts": [], "citingPaper": {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "externalIds": {"ArXiv": "2305.14788", "DBLP": "journals/corr/abs-2305-14788", "DOI": "10.48550/arXiv.2305.14788", "CorpusId": 258865249}, "url": "https://www.semanticscholar.org/paper/2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts", "abstract": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts.", "venue": "arXiv.org", "year": 2023, "citationCount": 3, "influentialCitationCount": 1, "authors": [{"authorId": "2161343103", "name": "Alexis Chevalier"}, {"authorId": "2127066887", "name": "Alexander Wettig"}, {"authorId": "2218438150", "name": "Anirudh Ajith"}, {"authorId": "50536468", "name": "Danqi Chen"}]}}, {"contexts": [], "citingPaper": {"paperId": "87177a718bb12d9dc7389e7762bc859730803b91", "externalIds": {"ArXiv": "2305.15275", "DBLP": "journals/corr/abs-2305-15275", "DOI": "10.48550/arXiv.2305.15275", "CorpusId": 258865774}, "url": "https://www.semanticscholar.org/paper/87177a718bb12d9dc7389e7762bc859730803b91", "title": "Self-Evolution Learning for Discriminative Language Model Pretraining", "abstract": "Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning (SE), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. SE focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our SE brings consistent and significant improvements (+1.43~2.12 average scores) upon different PLMs. In-depth analyses demonstrate that SE improves linguistic knowledge learning and generalization.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2114810150", "name": "Qihuang Zhong"}, {"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "46701032", "name": "Juhua Liu"}, {"authorId": "2064619959", "name": "Bo Du"}, {"authorId": "2140448089", "name": "Dacheng Tao"}]}}, {"contexts": ["\u2026al., 2018b), wsc (Levesque et al., 2012); reading comprehension: raceh, racem\n(Lai et al., 2017); cloze completion: storycloze (Mostafazadeh et al., 2017), natural language inference (NLI): anli-r{1,2,3} (Nie et al., 2020), rte (Wang et al., 2018, 2019), wic (Pilehvar and Camacho-Collados, 2019)."], "citingPaper": {"paperId": "ede3a405ae7f093fa2331075e22e29fca9e11f7e", "externalIds": {"ArXiv": "2305.14926", "DBLP": "journals/corr/abs-2305-14926", "DOI": "10.48550/arXiv.2305.14926", "CorpusId": 258866077}, "url": "https://www.semanticscholar.org/paper/ede3a405ae7f093fa2331075e22e29fca9e11f7e", "title": "Universal Self-adaptive Prompting", "abstract": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through prompt-based and/or in-context learning. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data&an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types, and then uses a corresponding selector to select the most suitable queries&zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate zero-shot USP with two PaLM models, and demonstrate performances that are considerably stronger than standard zero-shot baselines and are comparable to or even superior than few-shot baselines across more than 20 natural language understanding (NLU) and natural language generation (NLG) tasks.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "1470501980", "name": "Xingchen Wan"}, {"authorId": "2068169921", "name": "Ruoxi Sun"}, {"authorId": "3346298", "name": "Hootan Nakhost"}, {"authorId": "2791430", "name": "H. Dai"}, {"authorId": "117595858", "name": "Julian Martin Eisenschlos"}, {"authorId": "2676352", "name": "Sercan \u00d6. Arik"}, {"authorId": "1945962", "name": "Tomas Pfister"}]}}, {"contexts": [], "citingPaper": {"paperId": "acbabae091bed349535537b05a9ee80467faf255", "externalIds": {"DBLP": "journals/corr/abs-2305-13256", "ArXiv": "2305.13256", "DOI": "10.48550/arXiv.2305.13256", "CorpusId": 258832390}, "url": "https://www.semanticscholar.org/paper/acbabae091bed349535537b05a9ee80467faf255", "title": "TaskWeb: Selecting Better Source Tasks for Multi-task NLP", "abstract": "Recent work in NLP has shown promising results in training models on large amounts of tasks to achieve better generalization. However, it is not well-understood how tasks are related, and how helpful training tasks can be chosen for a new task. In this work, we investigate whether knowing task relationships via pairwise task transfer improves choosing one or more source tasks that help to learn a new target task. We provide TaskWeb, a large-scale benchmark of pairwise task transfers for 22 NLP tasks using three different model types, sizes, and adaptation methods, spanning about 25,000 experiments. Then, we design a new method TaskShop based on our analysis of TaskWeb. TaskShop uses TaskWeb to estimate the benefit of using a source task for learning a new target, and to choose a subset of helpful training tasks for multi-task learning. Our method improves overall rankings and top-k precision of source tasks by 12% and 29%, respectively. We also use TaskShop to build smaller multi-task training sets that improve zero-shot performances across 11 different target tasks by at least 4.3%.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2117060176", "name": "Joongwon Kim"}, {"authorId": "35584853", "name": "Akari Asai"}, {"authorId": "1387994137", "name": "Gabriel Ilharco"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}]}}, {"contexts": ["For English WiC, the word senses of WordNet (Miller, 1994) are used to determine sense equivalence (Pilehvar and Camacho-Collados, 2019).", "Word Sense Disambiguation (WSD) As our Word Sense Disambiguation (WSD) task, we replicate the English Words in Context (WiC; Pilehvar and Camacho-Collados, 2019) task."], "citingPaper": {"paperId": "f8ca25b724c3df79aefb90af41c84fb074df2621", "externalIds": {"DBLP": "journals/corr/abs-2305-13026", "ArXiv": "2305.13026", "DOI": "10.48550/arXiv.2305.13026", "CorpusId": 258832821}, "url": "https://www.semanticscholar.org/paper/f8ca25b724c3df79aefb90af41c84fb074df2621", "title": "DUMB: A Benchmark for Smart Evaluation of Dutch Models", "abstract": "We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of eight tasks include three tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of models to a strong baseline which can be referred to in the future even when assessing different sets of models. Through a comparison of 14 pre-trained models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition to highlighting best strategies for training larger Dutch models, DUMB will foster further research on Dutch. A public leaderboard is available at https://dumbench.nl.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "144611157", "name": "Wietse de Vries"}, {"authorId": "1811668", "name": "Martijn Wieling"}, {"authorId": "2742475", "name": "M. Nissim"}]}}, {"contexts": [], "citingPaper": {"paperId": "1d98e3de197c56ff89754fb4423418d8df5e931f", "externalIds": {"DBLP": "journals/corr/abs-2305-12576", "ArXiv": "2305.12576", "DOI": "10.48550/arXiv.2305.12576", "CorpusId": 258832375}, "url": "https://www.semanticscholar.org/paper/1d98e3de197c56ff89754fb4423418d8df5e931f", "title": "Automated Few-shot Classification with Instruction-Finetuned Language Models", "abstract": "A particularly successful class of approaches for few-shot learning combines language models with prompts -- hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models exhibit remarkable prompt robustness, and we subsequently propose a simple method to eliminate the need for handcrafted prompts, named AuT-Few. This approach consists of (i) a prompt retrieval module that selects suitable task instructions from the instruction-tuning knowledge base, and (ii) the generation of two distinct, semantically meaningful, class descriptions and a selection mechanism via cross-validation. Over $12$ datasets, spanning $8$ classification tasks, we show that AuT-Few outperforms current state-of-the-art few-shot learning methods. Moreover, AuT-Few is the best ranking method across datasets on the RAFT few-shot benchmark. Notably, these results are achieved without task-specific handcrafted prompts on unseen tasks.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "134617539", "name": "Rami Aly"}, {"authorId": "2110332219", "name": "Xingjian Shi"}, {"authorId": "3002019", "name": "Kaixiang Lin"}, {"authorId": "2085709", "name": "Aston Zhang"}, {"authorId": "145771261", "name": "A. Wilson"}]}}, {"contexts": [], "citingPaper": {"paperId": "a43438b65660e69f7c7341b5f3ced15d5ac98c8d", "externalIds": {"DBLP": "journals/corr/abs-2305-10284", "ArXiv": "2305.10284", "DOI": "10.48550/arXiv.2305.10284", "CorpusId": 258741244}, "url": "https://www.semanticscholar.org/paper/a43438b65660e69f7c7341b5f3ced15d5ac98c8d", "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "abstract": "The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2217680859", "name": "Anas Himmi"}, {"authorId": "2217198", "name": "Ekhine Irurozki"}, {"authorId": "88972690", "name": "Nathan Noiry"}, {"authorId": "1696620", "name": "S. Cl\u00e9men\u00e7on"}, {"authorId": "46985469", "name": "Pierre Colombo"}]}}, {"contexts": [], "citingPaper": {"paperId": "a3843d95baf0beee6aa376c56d09368d11811eb5", "externalIds": {"DBLP": "conf/acl/Wannasuphoprasit23", "ArXiv": "2305.10610", "DOI": "10.48550/arXiv.2305.10610", "CorpusId": 258762666}, "url": "https://www.semanticscholar.org/paper/a3843d95baf0beee6aa376c56d09368d11811eb5", "title": "Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting", "abstract": "Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words (Zhou et al., 2022). This similarity underestimation problem is particularly severe for highly frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the L2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus. Consequently, the larger L2 norms associated with the highly frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores. To solve this issue, we propose a method to discount the L2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words. We show that the so called stop words behave differently from the rest of the words, which require special consideration during their discounting process. Experimental results on a contextualised word similarity dataset show that our proposed discounting method accurately solves the similarity underestimation problem.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2217758535", "name": "Saeth Wannasuphoprasit"}, {"authorId": "32066669", "name": "Yi Zhou"}, {"authorId": "2075356592", "name": "D. Bollegala"}]}}, {"contexts": [], "citingPaper": {"paperId": "90df9c6924425d7366d16731a34bfa4e52ad5b2e", "externalIds": {"DBLP": "conf/acl/TedeschiBDHHHKK23", "ACL": "2023.acl-long.697", "ArXiv": "2305.08414", "DOI": "10.48550/arXiv.2305.08414", "CorpusId": 258686286}, "url": "https://www.semanticscholar.org/paper/90df9c6924425d7366d16731a34bfa4e52ad5b2e", "title": "What\u2019s the Meaning of Superhuman Performance in Today\u2019s NLU?", "abstract": "In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 3, "influentialCitationCount": 0, "authors": [{"authorId": "2140370472", "name": "Simone Tedeschi"}, {"authorId": "3461596", "name": "Johan Bos"}, {"authorId": "72836788", "name": "T. Declerck"}, {"authorId": "144002335", "name": "Jan Hajic"}, {"authorId": "2064295987", "name": "Daniel Hershcovich"}, {"authorId": "144547315", "name": "E. Hovy"}, {"authorId": "145542037", "name": "Alexander Koller"}, {"authorId": "1755311", "name": "Simon Krek"}, {"authorId": "2265382", "name": "S. Schockaert"}, {"authorId": "2082372", "name": "Rico Sennrich"}, {"authorId": "2362276", "name": "Ekaterina Shutova"}, {"authorId": "1733928", "name": "Roberto Navigli"}]}}, {"contexts": [], "citingPaper": {"paperId": "82b17686abba18dfc821a262dab5fbb081aa2388", "externalIds": {"DBLP": "journals/corr/abs-2305-03937", "ArXiv": "2305.03937", "DOI": "10.48550/arXiv.2305.03937", "CorpusId": 258557553}, "url": "https://www.semanticscholar.org/paper/82b17686abba18dfc821a262dab5fbb081aa2388", "title": "Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization", "abstract": "Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute<0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning on SuperGLUE benchmark. Notably, our method reaches +7 points improvement over prompt tuning with T5-Base and allows to reduce the prompt length by 10x without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot settings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "1397302840", "name": "Anastasia Razdaibiedina"}, {"authorId": "3375249", "name": "Yuning Mao"}, {"authorId": "2132302721", "name": "Rui Hou"}, {"authorId": "2072010", "name": "Madian Khabsa"}, {"authorId": "35084211", "name": "M. Lewis"}, {"authorId": "2503659", "name": "Jimmy Ba"}, {"authorId": "2634674", "name": "Amjad Almahairi"}]}}, {"contexts": ["In all experiments, every model is fine-tuned separately three times for each of the datasets in the SuperGLUE benchmark: BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), COPA (Roemmele et al., 2011), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE (Bar-Haim et al., 2006; Bentivogli et al., 2009; Dagan et al., 2006; Giampiccolo et al., 2007), WiC (Pilehvar & Camacho-Collados, 2019), and WSC (Levesque et al., 2012).", "\u2026CB (de Marneffe et al., 2019), COPA (Roemmele et al., 2011), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE (Bar-Haim et al., 2006; Bentivogli et al., 2009; Dagan et al., 2006; Giampiccolo et al., 2007), WiC (Pilehvar & Camacho-Collados, 2019), and WSC (Levesque et al., 2012)."], "citingPaper": {"paperId": "060c886bcf59508b61946646fc1a12ac449cfdfb", "externalIds": {"ArXiv": "2305.03232", "DBLP": "journals/corr/abs-2305-03232", "DOI": "10.48550/arXiv.2305.03232", "CorpusId": 258546998}, "url": "https://www.semanticscholar.org/paper/060c886bcf59508b61946646fc1a12ac449cfdfb", "title": "Neuromodulation Gated Transformer", "abstract": "We introduce a novel architecture, the Neuromodulation Gated Transformer (NGT), which is a simple implementation of neuromodulation in transformers via a multiplicative effect. We compare it to baselines and show that it results in the best average performance on the SuperGLUE benchmark validation sets.", "venue": "Tiny Papers @ ICLR", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2216490249", "name": "Kobe Knowles"}, {"authorId": "6835173", "name": "Joshua Bensemann"}, {"authorId": "2992071", "name": "Diana Benavides Prado"}, {"authorId": "51071778", "name": "Vithya Yogarajan"}, {"authorId": "2582677", "name": "Michael Witbrock"}, {"authorId": "152945656", "name": "G. Dobbie"}, {"authorId": "2144353312", "name": "Yang Chen"}]}}, {"contexts": [], "citingPaper": {"paperId": "28f7e4cf501e97f7940243452d817381635e55f8", "externalIds": {"DBLP": "journals/corr/abs-2305-02423", "ArXiv": "2305.02423", "DOI": "10.48550/arXiv.2305.02423", "CorpusId": 258479677}, "url": "https://www.semanticscholar.org/paper/28f7e4cf501e97f7940243452d817381635e55f8", "title": "PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer", "abstract": "Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can cause a big fluctuation in the loss landscape. This is an essential factor that leads to the instability of prompt tuning. Based on this observation, we introduce perturbation-based regularizers, which can smooth the loss landscape, into prompt tuning. We propose a new algorithm, called Prompt Tuning with Perturbation-based regularizer~(PTP), which can not only alleviate training instability dramatically but also boost the performance of prompt tuning. We design two kinds of perturbation-based regularizers, including random-noise-based and adversarial-based. In particular, our proposed perturbations are flexible on both text space and embedding space. Extensive experiments show the effectiveness of our proposed methods in stabilizing the training. Our new algorithms improve the state-of-the-art prompt tuning methods by 1.94\\% and 2.34\\% on SuperGLUE and FewGLUE benchmarks, respectively.", "venue": "arXiv.org", "year": 2023, "citationCount": 3, "influentialCitationCount": 0, "authors": [{"authorId": "2108451006", "name": "Lichang Chen"}, {"authorId": "2214208405", "name": "Heng Huang"}, {"authorId": "2424698", "name": "Minhao Cheng"}]}}, {"contexts": ["Specifically, we utilize the sense definitions of an ambiguous word that have been widely exploited in previous lexical semantic tasks (Raganato et al., 2017; Gella et al., 2017; Pilehvar and Camacho-Collados, 2019).", "In our work, we utilize WordNet (Miller, 1995) which has been widely used in previous semantic analysis tasks (Pilehvar and Camacho-Collados, 2019; Bevilacqua et al., 2021) as our source of LKB."], "citingPaper": {"paperId": "9658957492abb9a627337fffd467b78cb9a0607d", "externalIds": {"DBLP": "conf/acl/KwonGL0023", "ACL": "2023.acl-long.88", "ArXiv": "2305.01788", "DOI": "10.48550/arXiv.2305.01788", "CorpusId": 258461036}, "url": "https://www.semanticscholar.org/paper/9658957492abb9a627337fffd467b78cb9a0607d", "title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information", "abstract": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "12693064", "name": "Sunjae Kwon"}, {"authorId": "2215923133", "name": "Rishabh Garodia"}, {"authorId": "2187932371", "name": "Minhwa Lee"}, {"authorId": "48598711", "name": "Zhichao Yang"}, {"authorId": "145174375", "name": "Hong Yu"}]}}, {"contexts": [], "citingPaper": {"paperId": "91125be6b5e2df94c7a48893bab2d1a7a7a19199", "externalIds": {"DBLP": "journals/corr/abs-2304-14106", "ArXiv": "2304.14106", "DOI": "10.48550/arXiv.2304.14106", "CorpusId": 258352238}, "url": "https://www.semanticscholar.org/paper/91125be6b5e2df94c7a48893bab2d1a7a7a19199", "title": "ChatLog: Recording and Analyzing ChatGPT Across Time", "abstract": "While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our project at https://github.com/THU-KEG/ChatLog.", "venue": "arXiv.org", "year": 2023, "citationCount": 6, "influentialCitationCount": 0, "authors": [{"authorId": "2116520118", "name": "Shangqing Tu"}, {"authorId": "2215518948", "name": "Chunyang Li"}, {"authorId": "2116034394", "name": "Jifan Yu"}, {"authorId": "48631777", "name": "Xiaozhi Wang"}, {"authorId": "2055765060", "name": "Lei Hou"}, {"authorId": "2133353675", "name": "Juanzi Li"}]}}, {"contexts": [], "citingPaper": {"paperId": "389ec3e8902a5dcfcde1adec735854e93f845937", "externalIds": {"ArXiv": "2304.14402", "DBLP": "journals/corr/abs-2304-14402", "DOI": "10.48550/arXiv.2304.14402", "CorpusId": 258352678}, "url": "https://www.semanticscholar.org/paper/389ec3e8902a5dcfcde1adec735854e93f845937", "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions", "abstract": "Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs into much smaller ones. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizable, we design our instructions to cover a broad set of topics to ensure diversity. Extensive analysis of our instruction dataset confirms its diversity, and we generate responses for these instructions using gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of models, collectively referred to as LaMini-LM, which includes models from both the encoder-decoder and decoder-only families, with varying sizes. We evaluate the performance of our models using automatic metrics on 15 different natural language processing (NLP) benchmarks, as well as through human assessment. The results demonstrate that our proposed LaMini-LM models are comparable to competitive baselines, while being nearly 10 times smaller in size.", "venue": "arXiv.org", "year": 2023, "citationCount": 11, "influentialCitationCount": 2, "authors": [{"authorId": "2145209409", "name": "Minghao Wu"}, {"authorId": "2215440985", "name": "Abdul Waheed"}, {"authorId": "50445559", "name": "Chiyu Zhang"}, {"authorId": "2065312024", "name": "M. Abdul-Mageed"}, {"authorId": "8129718", "name": "Alham Fikri Aji"}]}}, {"contexts": [], "citingPaper": {"paperId": "846db95f3c330d777f76692dcc7534dc364149c8", "externalIds": {"ArXiv": "2304.13803", "DBLP": "journals/corr/abs-2304-13803", "DOI": "10.48550/arXiv.2304.13803", "CorpusId": 258352741}, "url": "https://www.semanticscholar.org/paper/846db95f3c330d777f76692dcc7534dc364149c8", "title": "Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models", "abstract": "Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can be finetuned to perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD). However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best leverage the cross-lingual knowledge inside PLMs for robust zero-shot reasoning in any language.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2215493462", "name": "Haoqiang Kang"}, {"authorId": "3443287", "name": "Terra Blevins"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}}, {"contexts": [", 2022) or Word-in-Context task (Pilehvar and CamachoCollados, 2019; Martelli et al., 2021) will bring us more comprehensive insights on the effectiveness and applicability."], "citingPaper": {"paperId": "125aeb18a0e4b6eb75b09b2cebddf7b73e062809", "externalIds": {"ACL": "2023.eacl-main.251", "DBLP": "conf/eacl/MizukiO23", "ArXiv": "2304.11340", "DOI": "10.48550/arXiv.2304.11340", "CorpusId": 258297957}, "url": "https://www.semanticscholar.org/paper/125aeb18a0e4b6eb75b09b2cebddf7b73e062809", "title": "Semantic Specialization for Knowledge-based Word Sense Disambiguation", "abstract": "A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the sense and context embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-based WSD when combined with the reranking heuristic that uses the sense inventory. We found that the similarity characteristics of specialized embeddings conform to the key idea. We also found that the (dis)similarity of embeddings between the related/different/unrelated senses correlates well with the performance of WSD.", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "98889783", "name": "Sakae Mizuki"}, {"authorId": "1764004", "name": "Naoaki Okazaki"}]}}, {"contexts": [], "citingPaper": {"paperId": "1f0dfbbc13ac31de8709bbb4d0f6478aa1222cef", "externalIds": {"ArXiv": "2304.09402", "DBLP": "journals/corr/abs-2304-09402", "DOI": "10.48550/arXiv.2304.09402", "CorpusId": 258212706}, "url": "https://www.semanticscholar.org/paper/1f0dfbbc13ac31de8709bbb4d0f6478aa1222cef", "title": "MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning", "abstract": "Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2165246108", "name": "Bohan Li"}, {"authorId": "49093992", "name": "Longxu Dou"}, {"authorId": "8471176", "name": "Yutai Hou"}, {"authorId": "1419728983", "name": "Yunlong Feng"}, {"authorId": "11322334", "name": "Honglin Mu"}, {"authorId": "2256319", "name": "Wanxiang Che"}]}}, {"contexts": [], "citingPaper": {"paperId": "5fb52f4f2b65eb4d5eea7a2d105937ccc778b949", "externalIds": {"DBLP": "journals/corr/abs-2304-06762", "ArXiv": "2304.06762", "DOI": "10.48550/arXiv.2304.06762", "CorpusId": 258170263}, "url": "https://www.semanticscholar.org/paper/5fb52f4f2b65eb4d5eea7a2d105937ccc778b949", "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study", "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT across different model sizes. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our implementation at: https://github.com/NVIDIA/Megatron-LM#retro", "venue": "arXiv.org", "year": 2023, "citationCount": 5, "influentialCitationCount": 1, "authors": [{"authorId": "51454501", "name": "Boxin Wang"}, {"authorId": "2056440915", "name": "Wei Ping"}, {"authorId": "2092025743", "name": "P. Xu"}, {"authorId": "20957879", "name": "Lawrence C. McAfee"}, {"authorId": "152613855", "name": "Zihan Liu"}, {"authorId": "1911755", "name": "M. Shoeybi"}, {"authorId": "2156155948", "name": "Yi Dong"}, {"authorId": "2787022", "name": "Oleksii Kuchaiev"}, {"authorId": "71788673", "name": "Bo Li"}, {"authorId": "2723309", "name": "Chaowei Xiao"}, {"authorId": "2047844", "name": "Anima Anandkumar"}, {"authorId": "2301680", "name": "Bryan Catanzaro"}]}}, {"contexts": [], "citingPaper": {"paperId": "93752cae0d4ecd2d09d6660feb3c1860af973f18", "externalIds": {"ArXiv": "2305.03514", "DBLP": "journals/corr/abs-2305-03514", "DOI": "10.48550/arXiv.2305.03514", "CorpusId": 258547324}, "url": "https://www.semanticscholar.org/paper/93752cae0d4ecd2d09d6660feb3c1860af973f18", "title": "Can Large Language Models Transform Computational Social Science?", "abstract": "Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the hidden meaning behind text). In summary, LLMs can significantly reduce costs and increase efficiency of social science analysis in partnership with humans.", "venue": "arXiv.org", "year": 2023, "citationCount": 29, "influentialCitationCount": 3, "authors": [{"authorId": "1399135100", "name": "Caleb Ziems"}, {"authorId": "46552910", "name": "William B. Held"}, {"authorId": "1380218838", "name": "Omar Shaikh"}, {"authorId": "47739850", "name": "Jiaao Chen"}, {"authorId": "2109462667", "name": "Zhehao Zhang"}, {"authorId": "2143919864", "name": "Diyi Yang"}]}}, {"contexts": ["In Arefyev et al. [2021], ADP is employed to measure the shift sw over the embeddings \u03a61w and \u03a6 2 w extracted from a supervised Word-in-Context model (WiC) [Pilehvar and Camacho-Collados, 2019].", "[2021], ADP is employed to measure the shift sw over the embeddings \u03a6(1)w and \u03a6 2 w extracted from a supervised Word-in-Context model (WiC) [Pilehvar and Camacho-Collados, 2019]."], "citingPaper": {"paperId": "ab08495f575c7616b316b86a5e4cdbe84fb5d0e6", "externalIds": {"DBLP": "journals/corr/abs-2304-01666", "ArXiv": "2304.01666", "DOI": "10.48550/arXiv.2304.01666", "CorpusId": 257921251}, "url": "https://www.semanticscholar.org/paper/ab08495f575c7616b316b86a5e4cdbe84fb5d0e6", "title": "A Survey on Contextualised Semantic Shift Detection", "abstract": "Semantic Shift Detection (SSD) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, SSD has been addressed by linguists and social scientists through manual and time-consuming activities. In the recent years, computational approaches based on Natural Language Processing and word embeddings gained increasing attention to automate SSD as much as possible. In particular, over the past three years, significant advancements have been made almost exclusively based on word contextualised embedding models, which can handle the multiple usages/meanings of the words and better capture the related semantic shifts. In this paper, we survey the approaches based on contextualised embeddings for SSD (i.e., CSSDetection) and we propose a classification framework characterised by meaning representation, time-awareness, and learning modality dimensions. The framework is exploited i) to review the measures for shift assessment, ii) to compare the approaches on performance, and iii) to discuss the current issues in terms of scalability, interpretability, and robustness. Open challenges and future research directions about CSSDetection are finally outlined.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 1, "authors": [{"authorId": "1732265", "name": "S. Montanelli"}, {"authorId": "2136116967", "name": "Francesco Periti"}]}}, {"contexts": [], "citingPaper": {"paperId": "70da4fb798a86cbe8cad96c27ced0415885bbd9d", "externalIds": {"ArXiv": "2303.16854", "DBLP": "journals/corr/abs-2303-16854", "DOI": "10.48550/arXiv.2303.16854", "CorpusId": 257805087}, "url": "https://www.semanticscholar.org/paper/70da4fb798a86cbe8cad96c27ced0415885bbd9d", "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators", "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation.", "venue": "arXiv.org", "year": 2023, "citationCount": 19, "influentialCitationCount": 0, "authors": [{"authorId": "1754500", "name": "Xingwei He"}, {"authorId": "31113759", "name": "Zheng-Wen Lin"}, {"authorId": "2171182", "name": "Yeyun Gong"}, {"authorId": "15796861", "name": "Alex Jin"}, {"authorId": "2119077859", "name": "Hang Zhang"}, {"authorId": "2186278677", "name": "Chen Lin"}, {"authorId": "2143968416", "name": "Jian Jiao"}, {"authorId": "2184000509", "name": "S. Yiu"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "2109136147", "name": "Weizhu Chen"}]}}, {"contexts": ["\u2022 Words in Context (WIC Pilehvar and Camacho-Collados, 2019): Determine if a word is being used with the same meaning in two sentences."], "citingPaper": {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "externalIds": {"DBLP": "journals/corr/abs-2303-17564", "ArXiv": "2303.17564", "DOI": "10.48550/arXiv.2303.17564", "CorpusId": 257833842}, "url": "https://www.semanticscholar.org/paper/83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance", "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.", "venue": "arXiv.org", "year": 2023, "citationCount": 74, "influentialCitationCount": 8, "authors": [{"authorId": "50425845", "name": "Shijie Wu"}, {"authorId": "2329943", "name": "Ozan Irsoy"}, {"authorId": "2144851146", "name": "Steven Lu"}, {"authorId": "2213265733", "name": "Vadim Dabravolski"}, {"authorId": "1782853", "name": "Mark Dredze"}, {"authorId": "3159346", "name": "Sebastian Gehrmann"}, {"authorId": "2071549", "name": "P. Kambadur"}, {"authorId": "2064188376", "name": "D. Rosenberg"}, {"authorId": "2065970358", "name": "Gideon Mann"}]}}, {"contexts": ["We consider 6 downstream tasks with 12 datasets: 1) the sentiment analysis datasets SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), MR (Pang and Lee, 2005) and CR (Hu and Liu, 2004); 2) the subjectivity classification dataset SUBJ (Pang and Lee, 2004); 3) the natural language inference datasets CB (De Marneffe et al., 2019) and RTE (Wang et al., 2019); 4) the question answering dataset QNLI (Rajpurkar et al., 2016) and BoolQ (Clark et al., 2019); 5) the word sense disambiguation dataset WiC (Pilehvar and Camacho-Collados, 2019); 6) the paraphrase detection datasets MRPC (Dolan and Brockett, 2005) and QQP.1\nFollowing Karimi Mahabadi et al. (2022), for each dataset we sample 16 instances per label from the original training set to form training and validation sets for few-shot learning.", "\u20262019); 4) the question answering dataset QNLI (Rajpurkar et al., 2016) and BoolQ (Clark et al., 2019); 5) the word sense disambiguation dataset WiC (Pilehvar and Camacho-Collados, 2019); 6) the paraphrase detection datasets MRPC (Dolan and Brockett, 2005) and QQP.1\nFollowing Karimi Mahabadi et al.\u2026", "In few-shot learning, following some prior work (Schick and Sch\u00fctze, 2021; Karimi Mahabadi et al., 2022), we set the maximum sequence length of each example to 256 for CR, SUBJ, CB, RTE and WiC, and 128 for other datasets.", "original validation set to construct test set for SST2, CB, RTE, QNLI, BoolQ, WiC, MRPC, and QQP."], "citingPaper": {"paperId": "2c503f430a7bff9e7ba13e36dd7ae3a5df7c8814", "externalIds": {"ArXiv": "2303.12314", "CorpusId": 257663696}, "url": "https://www.semanticscholar.org/paper/2c503f430a7bff9e7ba13e36dd7ae3a5df7c8814", "title": "Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization", "abstract": "Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-supervised meta-prompt learning framework with meta-gradient regularization for few-shot generalization (SUPMER). SUPMER leverages self-supervised meta-learning with a diverse set of well-designed meta-tasks to learn a universal prompt initialization for efficient adaptation using only unlabeled data. Additionally, it jointly meta-learns a gradient regularization function to transform raw gradients into a domain-generalizable direction, thus alleviating the problem of overfitting. Extensive experiments show that SUPMER achieves better performance for different few-shot downstream tasks, and also exhibits a stronger domain generalization ability.", "venue": "", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2212175601", "name": "Kaihang Pan"}, {"authorId": "3428237", "name": "Juncheng Billy Li"}, {"authorId": "2122383983", "name": "Hongye Song"}, {"authorId": "2110808728", "name": "Jun Lin"}, {"authorId": "1713802", "name": "Xiaozhong Liu"}, {"authorId": "2118071462", "name": "Siliang Tang"}]}}, {"contexts": [], "citingPaper": {"paperId": "a713d9fb97c1e2b3982bc2f71128989c085d3f13", "externalIds": {"ArXiv": "2303.09859", "ACL": "2023.findings-eacl.146", "DBLP": "conf/eacl/SamuelKOV23", "DOI": "10.48550/arXiv.2303.09859", "CorpusId": 257622986}, "url": "https://www.semanticscholar.org/paper/a713d9fb97c1e2b3982bc2f71128989c085d3f13", "title": "Trained on 100 million words and still in shape: BERT meets British National Corpus", "abstract": "While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source \u2013 the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.", "venue": "Findings", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "authors": [{"authorId": "2064321898", "name": "David Samuel"}, {"authorId": "2689095", "name": "Andrey Kutuzov"}, {"authorId": "2732223", "name": "Lilja \u00d8vrelid"}, {"authorId": "2027091", "name": "Erik Velldal"}]}}, {"contexts": ["E2E WebNLG BLEU NIST METEOR Rouge-L CIDEr BLEU METEOR TER (\u2193)\nPT 29.11 5.00 0.343 51.50 1.72 46.02 0.37 46.89 MPT 32.14 5.35 0.363 52.88 1.86 52.27 0.40 41.36\nT5-S mal l T5-B ase T5-L arge\nWiC\n40\n50\n60\n70\n80\n90\nAc cu\nra cy\nT5-S mal l T5-B ase T5-L arge\nMultiRC\n40\n50\n60\n70\n80\n90\nF1\nT5-S mal l T5-B ase T5-L arge\nBoolQ\n40\n50\n60\n70\n80\n90\nAc cu\nra cy\nFT Adapter PT ATTEMPT MPT\nMul tiRC Boo lQ WiC WSC CB\nSPoT\nMultiRC\nBoolQ\nWiC\nWSC\nCB\nMul tiRC Boo lQ WiC WSC CB\nMPT\nMultiRC\nBoolQ\nWiC\nWSC\nCB\nFigure 4: (Left) Performance of various baselines as a function of model size (from T5-Small to T5-Large).", "We use 23 datasets from four benchmarks as target tasks: MultiRC (Khashabi et al., 2018), BoolQ (Clark et al., 2019a), WiC (Pilehvar & Camacho-Collados, 2018), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019) from SuperGLUE (Wang et al., 2019); RTE (Giampiccolo et al., 2007), CoLA\u2026", "We use 23 datasets from four benchmarks as target tasks: MultiRC (Khashabi et al., 2018), BoolQ (Clark et al., 2019a), WiC (Pilehvar & Camacho-Collados, 2018), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019) from SuperGLUE (Wang et al., 2019); RTE (Giampiccolo et al., 2007), CoLA (Warstadt et al., 2019), STS-B (Cer et al., 2017), MRPC (Dolan & Brockett, 2005), MNLI, QQP, QNLI and SST-2 from GLUE (Wang et al., 2018); Natural Questions (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017) and SearchQA (Dunn et al., 2017) from MRQA (Fisch et al., 2019); WinoGrande (Sakaguchi et al., 2021), Yelp-2 (Zhang et al., 2015), SciTail (Khot et al., 2018) and PAWS-Wiki (Zhang et al., 2019) from the \u201cOthers\u201d benchmark in (Asai et al., 2022); and E2E (Novikova et al., 2017) and WebNLG (Gardent et al., 2017) for experiments on adapting to natural language generation tasks."], "citingPaper": {"paperId": "8b32aa33601514976d88fabcb060a5cd38d34006", "externalIds": {"DBLP": "conf/iclr/WangPKF0K23", "ArXiv": "2303.02861", "DOI": "10.48550/arXiv.2303.02861", "CorpusId": 257365136}, "url": "https://www.semanticscholar.org/paper/8b32aa33601514976d88fabcb060a5cd38d34006", "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning", "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters.", "venue": "International Conference on Learning Representations", "year": 2023, "citationCount": 10, "influentialCitationCount": 0, "authors": [{"authorId": "47197370", "name": "Zhen Wang"}, {"authorId": "1819152", "name": "R. Panda"}, {"authorId": "2142741421", "name": "Leonid Karlinsky"}, {"authorId": "1723233", "name": "R. Feris"}, {"authorId": "1515546612", "name": "Huan Sun"}, {"authorId": "152847918", "name": "Yoon Kim"}]}}, {"contexts": ["Following previous work [15], we evaluate our approach on five SuperGLUE [52] datasets to test the language understanding ability, namely BoolQ [10], MultiRC [28], CB [12], RTE [16], and WiC [44].", "E.6 Details of datasets\nFollowing previous work [15], we evaluate our approach on five SuperGlue [52] datasets to test the natural language understanding ability, namely BoolQ [10], MultiRC [28], CB [12], RTE [16], and WiC [44].", "Dataset T5-LM-Base T5-LM-Large\nFixed Length Adaptive Length Fixed Length Adaptive Ins_Length\nBoolq 62.35 67.28 81.20 83.46 MultiRC 57.41 57.34 58.00 66.30 WiC 53.61 60.50 69.30 71.47 CB 78.57 80.36 87.50 84.32 RTE 67.51 68.32 82.60 79.78 Avg."], "citingPaper": {"paperId": "6f75404b0d01f9a09afe428f9efd5cbcd7825469", "externalIds": {"DBLP": "journals/corr/abs-2303-02909", "ArXiv": "2303.02909", "DOI": "10.48550/arXiv.2303.02909", "CorpusId": 257365432}, "url": "https://www.semanticscholar.org/paper/6f75404b0d01f9a09afe428f9efd5cbcd7825469", "title": "Dynamic Prompting: A Unified Framework for Prompt Tuning", "abstract": "It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors of prompts based on specific tasks and instances. To accomplish this, we employ a lightweight learning network with Gumble-Softmax, allowing us to learn instance-dependent guidance. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP tasks, vision recognition tasks, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask scenarios. Codes are available at https://github.com/Xianjun-Yang/DPT.", "venue": "arXiv.org", "year": 2023, "citationCount": 3, "influentialCitationCount": 0, "authors": [{"authorId": "2145170944", "name": "Xianjun Yang"}, {"authorId": "145859270", "name": "Wei Cheng"}, {"authorId": "50879401", "name": "Xujiang Zhao"}, {"authorId": "21038849", "name": "Linda Petzold"}, {"authorId": "2145225543", "name": "Haifeng Chen"}]}}, {"contexts": ["The task of identifying the intended meaning of a word in a given context \u2013 Word in Context task (WIC) [71].", "3%) 1112 2 966 /149 7 WordContext S EN Word sense disambiguation Yes Binary pair classification WiC [71] / [72] 3 No 638 638 0 5 (0.", "1112 2 966 /149\n7 WordContext S EN Word sensedisambiguation Yes Binary pair classification WiC [71] / [72] 3 No 638 638 0 5 (0.8", "This phenomenon was observed mainly in WSD and WIC tasks.", "The WIC task is strongly related to the Word Sense Disambiguation task (WSD) as it tests language models\u2019 sense understanding abilities."], "citingPaper": {"paperId": "5848737f78397f72ceae2ba6f3419a6a8502b8ba", "externalIds": {"DBLP": "journals/corr/abs-2302-10724", "ArXiv": "2302.10724", "DOI": "10.1016/j.inffus.2023.101861", "CorpusId": 257050407}, "url": "https://www.semanticscholar.org/paper/5848737f78397f72ceae2ba6f3419a6a8502b8ba", "title": "ChatGPT: Jack of all trades, master of none", "abstract": null, "venue": "Information Fusion", "year": 2023, "citationCount": 52, "influentialCitationCount": 5, "authors": [{"authorId": "2169553526", "name": "Jan Koco'n"}, {"authorId": "2208975380", "name": "Igor Cichecki"}, {"authorId": "2208975873", "name": "Oliwier Kaszyca"}, {"authorId": "2208973923", "name": "Mateusz Kochanek"}, {"authorId": "2208973921", "name": "Dominika Szydlo"}, {"authorId": "2173806580", "name": "Joanna Baran"}, {"authorId": "2151200612", "name": "Julita Bielaniewicz"}, {"authorId": "2120757466", "name": "Marcin Gruza"}, {"authorId": "32559047", "name": "Arkadiusz Janz"}, {"authorId": "2007286374", "name": "Kamil Kanclerz"}, {"authorId": "2208972610", "name": "Anna Koco'n"}, {"authorId": "2208962106", "name": "Bartlomiej Koptyra"}, {"authorId": "1422481396", "name": "W. Mieleszczenko-Kowszewicz"}, {"authorId": "1413772109", "name": "P. Milkowski"}, {"authorId": "145050858", "name": "Marcin Oleksy"}, {"authorId": "144205338", "name": "Maciej Piasecki"}, {"authorId": "2208962428", "name": "Lukasz Radli'nski"}, {"authorId": "9583418", "name": "Konrad Wojtasik"}, {"authorId": "2208975638", "name": "Stanislaw Wo'zniak"}, {"authorId": "1724788", "name": "Przemyslaw Kazienko"}]}}, {"contexts": [], "citingPaper": {"paperId": "9af3b6b3f8dcedbb02b88936c428e1cd02503a8a", "externalIds": {"ACL": "2023.acl-long.659", "DBLP": "journals/corr/abs-2302-08143", "ArXiv": "2302.08143", "DOI": "10.48550/arXiv.2302.08143", "CorpusId": 256900985}, "url": "https://www.semanticscholar.org/paper/9af3b6b3f8dcedbb02b88936c428e1cd02503a8a", "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?", "abstract": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "authors": [{"authorId": "2084609980", "name": "Chengwei Qin"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "46428422", "name": "Q. Li"}, {"authorId": "2091437375", "name": "Ruochen Zhao"}]}}, {"contexts": [], "citingPaper": {"paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458", "externalIds": {"ArXiv": "2302.06675", "DBLP": "journals/corr/abs-2302-06675", "DOI": "10.48550/arXiv.2302.06675", "CorpusId": 256846990}, "url": "https://www.semanticscholar.org/paper/a7f59b2162ae0ea2520753b1b9b17277490a9458", "title": "Symbolic Discovery of Optimization Algorithms", "abstract": "We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.", "venue": "arXiv.org", "year": 2023, "citationCount": 49, "influentialCitationCount": 15, "authors": [{"authorId": "2143737082", "name": "Xiangning Chen"}, {"authorId": "145246869", "name": "Chen Liang"}, {"authorId": "2110408964", "name": "Da Huang"}, {"authorId": "2892780", "name": "Esteban Real"}, {"authorId": "4054249", "name": "Kaiyuan Wang"}, {"authorId": "2187205985", "name": "Yao Liu"}, {"authorId": "143950636", "name": "Hieu Pham"}, {"authorId": "9929684", "name": "Xuanyi Dong"}, {"authorId": "1821711", "name": "Thang Luong"}, {"authorId": "1793529", "name": "Cho-Jui Hsieh"}, {"authorId": "2141538599", "name": "Yifeng Lu"}, {"authorId": "1397917613", "name": "Quoc V. Le"}]}}, {"contexts": ["We choose SST-2 and SST-5 for sentiment classification (Socher et al., 2013), MNLI (Williams et al., 2018) for natural language inference, MultiRC (Khashabi et al., 2018) and BoolQ (Clark et al., 2019) for reading comprehension, AgNews (Zhang et al., 2015) for topic classification, WSC (Levesque et al., 2012) for coreference resolution, COPA (Roemmele et al., 2011) for commonsense reasoning and Trec (Hovy et al., 2001) along with WiC (Pilehvar & Camacho-Collados, 2019) for miscellaneous tasks.", "\u2026for reading comprehension, AgNews (Zhang et al., 2015) for topic classification, WSC (Levesque et al., 2012) for coreference resolution, COPA (Roemmele et al., 2011) for commonsense reasoning and Trec (Hovy et al., 2001) along with WiC (Pilehvar & Camacho-Collados, 2019) for miscellaneous tasks."], "citingPaper": {"paperId": "2e6fa3095df1d1ed041dfb4f5a18e31d4b7bd7bb", "externalIds": {"ArXiv": "2302.04931", "DBLP": "journals/corr/abs-2302-04931", "DOI": "10.48550/arXiv.2302.04931", "CorpusId": 256808462}, "url": "https://www.semanticscholar.org/paper/2e6fa3095df1d1ed041dfb4f5a18e31d4b7bd7bb", "title": "In-Context Learning with Many Demonstration Examples", "abstract": "Large pre-training language models (PLMs) have shown promising in-context learning abilities. However, due to the backbone transformer architecture, existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored. In this study, we propose a long-range language model EVALM based on an efficient transformer mechanism. EVALM is trained with 8k tokens per batch line and can test up to 256k-lengthed contexts with extrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based on EVALM, we scale up the size of examples efficiently in both instruction tuning and in-context learning to explore the boundary of the benefits from more annotated data. Experimental results on a diverse set of tasks show that EVALM achieves 4.1% higher accuracy on average, and the average length of achieving the best accuracy score over tasks is around 12k. We find that in-context learning can achieve higher performance with more demonstrations under many-shot instruction tuning (8k), and further extending the length of instructions (16k) can further improve the upper bound of scaling in-context learning.", "venue": "arXiv.org", "year": 2023, "citationCount": 6, "influentialCitationCount": 1, "authors": [{"authorId": "2027599235", "name": "Mukai Li"}, {"authorId": "2165001433", "name": "Shansan Gong"}, {"authorId": "2093485", "name": "Jiangtao Feng"}, {"authorId": "3032611", "name": "Yiheng Xu"}, {"authorId": "27672597", "name": "Jinchao Zhang"}, {"authorId": "150358371", "name": "Zhiyong Wu"}, {"authorId": "47648549", "name": "Lingpeng Kong"}]}}, {"contexts": ["General: CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), MRPC (Dolan & Brockett, 2005), QQP (data.quora.com/First-Quora-Dataset-Release-Question-Pairs), MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011) BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), CoPA (Roemmele et al., 2011), MULTIRC (Khashabi et al., 2018), WIC (Pilehvar & Camacho-Collados, 2019)\nNLI datasets: MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), ESNLI (Camburu et al., 2018), adversarial NLI (Nie et al., 2020).", "\u2026et al., 2011) BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), CoPA (Roemmele et al., 2011), MULTIRC (Khashabi et al., 2018), WIC (Pilehvar & Camacho-Collados, 2019)\nNLI datasets: MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al.,\u2026"], "citingPaper": {"paperId": "ca60e0fe07604095e74f85b34332f0310d18f579", "externalIds": {"DBLP": "journals/corr/abs-2302-04863", "ArXiv": "2302.04863", "DOI": "10.48550/arXiv.2302.04863", "CorpusId": 256697368}, "url": "https://www.semanticscholar.org/paper/ca60e0fe07604095e74f85b34332f0310d18f579", "title": "Knowledge is a Region in Weight Space for Fine-tuned Language Models", "abstract": "Research on neural networks has largely focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, especially those trained or tested on different datasets. We address this by studying how the weight space and underlying loss landscape of different models are interconnected. Specifically, we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also has high performance. Specifically, we show that language models that have been fine-tuned on the same dataset form a tight cluster in the weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning, even on tasks that the original models were not fine-tuned on. Our findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning. Specifically, we show that starting from the center of the region is as good or better than the pre-trained model in 11 of 12 datasets and improves accuracy by 3.06 on average.", "venue": "arXiv.org", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "authors": [{"authorId": "2204956989", "name": "Almog Gueta"}, {"authorId": "5598623", "name": "Elad Venezian"}, {"authorId": "2402716", "name": "Colin Raffel"}, {"authorId": "1766595", "name": "N. Slonim"}, {"authorId": "1722434", "name": "Yoav Katz"}, {"authorId": "41019330", "name": "Leshem Choshen"}]}}, {"contexts": [], "citingPaper": {"paperId": "86d03160e6f05deb17d0169e515f5a55d6361f7c", "externalIds": {"DBLP": "journals/corr/abs-2302-03202", "ArXiv": "2302.03202", "DOI": "10.48550/arXiv.2302.03202", "CorpusId": 256627673}, "url": "https://www.semanticscholar.org/paper/86d03160e6f05deb17d0169e515f5a55d6361f7c", "title": "Exploring the Benefits of Training Expert Language Models over Instruction Tuning", "abstract": "Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown the capability to generalize to unseen tasks. Previous work has shown that scaling the number of training tasks is the key component in making stronger MT LMs. In this work, we report an unexpected finding that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20% and 1.29%, respectively. This finding casts doubt on the previously held belief that simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding, we further show that this distributed approach of training a separate expert LM per training task instead of a single MT LM for zero-shot inference possesses many benefits including (1) avoiding negative task transfer that often occurs during instruction tuning, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together. The code is available at https://github.com/joeljang/ELM.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 18, "influentialCitationCount": 2, "authors": [{"authorId": "2000091730", "name": "Joel Jang"}, {"authorId": "2184037220", "name": "Seungone Kim"}, {"authorId": "2152111477", "name": "Seonghyeon Ye"}, {"authorId": "2180527259", "name": "Doyoung Kim"}, {"authorId": "2876316", "name": "L. Logeswaran"}, {"authorId": "3056520", "name": "Moontae Lee"}, {"authorId": "79733119", "name": "Kyungjae Lee"}, {"authorId": "4418074", "name": "Minjoon Seo"}]}}, {"contexts": ["\u2026(Sharma et al., 2018)), natural language inference (ANLI (Nie et al., 2020), CB (de Marneffe et al., 2019), RTE (Dagan et al., 2006)), coreference resolution (WSC (Levesque et al., 2012), Winogrande (Sakaguchi et al., 2020)), and word sense disambiguation (WiC (Pilehvar & Camacho-Collados, 2019))."], "citingPaper": {"paperId": "4bf68a5358453b651474f3c154ded652e0084a4f", "externalIds": {"DBLP": "journals/corr/abs-2302-00674", "ArXiv": "2302.00674", "DOI": "10.48550/arXiv.2302.00674", "CorpusId": 256459309}, "url": "https://www.semanticscholar.org/paper/4bf68a5358453b651474f3c154ded652e0084a4f", "title": "Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data", "abstract": "Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the combination of exploration and exploitation is crucial. Through extensive experimentation we find that our methods outperform all pre-existing FLAD methods by 4% and lead to the first 3 billion parameter language models that outperform the 175 billion parameter GPT-3. Overall, our work suggests that the discovery of better, more efficient mixing strategies for FLAD may provide a viable path towards substantially improving generalization in few-shot learning.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2044198106", "name": "Alon Albalak"}, {"authorId": "2402716", "name": "Colin Raffel"}, {"authorId": "1682479", "name": "William Yang Wang"}]}}, {"contexts": [", 2018) and Word-in-Context (WiC) (Pilehvar and Camacho-Collados, 2019)."], "citingPaper": {"paperId": "be261b8e681a1b147ada26242ff9a414030ec7c3", "externalIds": {"ArXiv": "2301.09820", "DBLP": "journals/corr/abs-2301-09820", "DOI": "10.48550/arXiv.2301.09820", "CorpusId": 256194226}, "url": "https://www.semanticscholar.org/paper/be261b8e681a1b147ada26242ff9a414030ec7c3", "title": "A Stability Analysis of Fine-Tuning a Pre-Trained Model", "abstract": "Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT, etc.) has proven to be one of the most promising paradigms in recent NLP research. However, numerous recent works indicate that fine-tuning suffers from the instability problem, i.e., tuning the same model under the same setting results in significantly different performance. Many recent works have proposed different methods to solve this problem, but there is no theoretical understanding of why and how these methods work. In this paper, we propose a novel theoretical stability analysis of fine-tuning that focuses on two commonly used settings, namely, full fine-tuning and head tuning. We define the stability under each setting and prove the corresponding stability bounds. The theoretical bounds explain why and how several existing methods can stabilize the fine-tuning procedure. In addition to being able to explain most of the observed empirical discoveries, our proposed theoretical analysis framework can also help in the design of effective and provable methods. Based on our theory, we propose three novel strategies to stabilize the fine-tuning procedure, namely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self Unsupervised Re-Training (SURT). We extensively evaluate our proposed approaches on 11 widely used real-world benchmark datasets, as well as hundreds of synthetic classification datasets. The experiment results show that our proposed methods significantly stabilize the fine-tuning procedure and also corroborate our theoretical analysis.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "1646716323", "name": "Z. Fu"}, {"authorId": "1734000", "name": "A. M. So"}, {"authorId": "50638196", "name": "Nigel Collier"}]}}, {"contexts": [], "citingPaper": {"paperId": "44d02cf6cd5ab64a2eaa3f724545ca849cecc164", "externalIds": {"ArXiv": "2301.03416", "DBLP": "journals/corr/abs-2301-03416", "DOI": "10.48550/arXiv.2301.03416", "CorpusId": 255546131}, "url": "https://www.semanticscholar.org/paper/44d02cf6cd5ab64a2eaa3f724545ca849cecc164", "title": "ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization", "abstract": "Task-agnostic knowledge distillation attempts to address the problem of deploying large pretrained language model in resource-constrained scenarios by compressing a large pretrained model called teacher into a smaller one called student such that the student can be directly finetuned on downstream tasks and retains comparable performance. However, we empirically find that there is a generalization gap between the student and the teacher in existing methods. In this work, we show that we can leverage multi-task learning in task-agnostic distillation to advance the generalization of the resulted student. In particular, we propose Multi-task Infused Task-agnostic Knowledge Distillation (MITKD). We first enhance the teacher by multi-task training it on multiple downstream tasks and then perform distillation to produce the student. Experimental results demonstrate that our method yields a student with much better generalization, significantly outperforms existing baselines, and establishes a new state-of-the-art result on in-domain, out-domain, and low-resource datasets in the setting of task-agnostic distillation. Moreover, our method even exceeds an 8x larger BERT$_{\\text{Base}}$ on SQuAD and four GLUE tasks. In addition, by combining ERNIE 3.0, our method achieves state-of-the-art results on 10 Chinese datasets.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2109563578", "name": "Weixin Liu"}, {"authorId": "2109214103", "name": "Xuyi Chen"}, {"authorId": "2144130913", "name": "Jiaxiang Liu"}, {"authorId": "144588144", "name": "Shi Feng"}, {"authorId": "2117103617", "name": "Yu Sun"}, {"authorId": "50007795", "name": "Hao Tian"}, {"authorId": "2190177674", "name": "Hua Wu"}]}}, {"contexts": ["\u2026RTE (Wang et al., 2019) (ii) coreference resolution: WinoGrande XL (ai2, 2019), WSC (Levesque et al., 2012) (iii) word sense disambiguation: WiC (Pilehvar and Camacho-Collados, 2019) (iv) sentence completion: COPA (Gordon et al., 2012), Hellaswag (Zellers et al., 2019) (v) question answering:\u2026", "We evaluate zero-shot generalization on 9 datasets in 5 traditional NLP tasks: (i) natural language inference: CB (de Marneffe et al., 2019), RTE (Wang et al., 2019) (ii) coreference resolution: WinoGrande XL (ai2, 2019), WSC (Levesque et al., 2012) (iii) word sense disambiguation: WiC (Pilehvar and Camacho-Collados, 2019) (iv) sentence completion: COPA (Gordon et al., 2012), Hellaswag (Zellers et al., 2019) (v) question answering: BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018)."], "citingPaper": {"paperId": "7a5cd8a1bf99c9cc58bd7a818be446c29e9e1cbb", "externalIds": {"ArXiv": "2212.10929", "DBLP": "journals/corr/abs-2212-10929", "DOI": "10.48550/arXiv.2212.10929", "CorpusId": 254926749}, "url": "https://www.semanticscholar.org/paper/7a5cd8a1bf99c9cc58bd7a818be446c29e9e1cbb", "title": "SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning", "abstract": "Pre-trained large language models can efficiently interpolate human-written prompts in a natural way. Multitask prompted learning can help generalization through a diverse set of tasks at once, thus enhancing the potential for more effective downstream fine-tuning. To perform efficient multitask-inference in the same batch, parameter-efficient fine-tuning methods such as prompt tuning have been proposed. However, the existing prompt tuning methods may lack generalization. We propose SPT, a semi-parametric prompt tuning method for multitask prompted learning. The novel component of SPT is a memory bank from where memory prompts are retrieved based on discrete prompts. Extensive experiments, such as (i) fine-tuning a full language model with SPT on 31 different tasks from 8 different domains and evaluating zero-shot generalization on 9 heldout datasets under 5 NLP task categories and (ii) pretraining SPT on the GLUE datasets and evaluating fine-tuning on the SuperGLUE datasets, demonstrate effectiveness of SPT.", "venue": "arXiv.org", "year": 2022, "citationCount": 4, "influentialCitationCount": 0, "authors": [{"authorId": "31773000", "name": "M Saiful Bari"}, {"authorId": "2085709", "name": "Aston Zhang"}, {"authorId": "2163381961", "name": "Shuai Zheng"}, {"authorId": "2110332219", "name": "Xingjian Shi"}, {"authorId": "2128816752", "name": "Yi Zhu"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "1701799", "name": "Mu Li"}]}}, {"contexts": [], "citingPaper": {"paperId": "8db5adadc0ab39f95a26a2eb6499d340d6c5ea21", "externalIds": {"ACL": "2023.cl-2.7", "DBLP": "journals/coling/Apidianaki23", "DOI": "10.1162/coli_a_00474", "CorpusId": 254960044}, "url": "https://www.semanticscholar.org/paper/8db5adadc0ab39f95a26a2eb6499d340d6c5ea21", "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation", "abstract": "Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.", "venue": "International Conference on Computational Logic", "year": 2022, "citationCount": 4, "influentialCitationCount": 0, "authors": [{"authorId": "2817917", "name": "Marianna Apidianaki"}]}}, {"contexts": [], "citingPaper": {"paperId": "5a3a8f962bd9d9320b49a921fc23756b3dfab6fc", "externalIds": {"DOI": "10.1556/2062.2022.00579", "CorpusId": 254611546}, "url": "https://www.semanticscholar.org/paper/5a3a8f962bd9d9320b49a921fc23756b3dfab6fc", "title": "A proof-of-concept meaning discrimination experiment to compile a word-in-context dataset for adjectives \u2013 A graph-based distributional approach", "abstract": "The Word-in-Context corpus, which forms part of the SuperGLUE benchmark dataset, focuses on a specific sense disambiguation task: it has to be decided whether two occurrences of a given target word in two different contexts convey the same meaning or not. Unfortunately, the WiC database exhibits a relatively low consistency in terms of inter-annotator agreement, which implies that the meaning discrimination task is not well defined even for humans. The present paper aims at tackling this problem through anchoring semantic information to observable surface data. For doing so, we have experimented with a graph-based distributional approach, where both sparse and dense adjectival vector representations served as input. According to our expectations the algorithm is able to anchor the semantic information to contextual data, and therefore it is able to provide clear and explicit criteria as to when the same meaning should be assigned to the occurrences. Moreover, since this method does not rely on any external knowledge base, it should be suitable for any low- or medium-resourced language.", "venue": "Acta Linguistica Academica", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2223806", "name": "Enik\u0151 H\u00e9ja"}, {"authorId": "1414711705", "name": "No\u00e9mi Ligeti-Nagy"}]}}, {"contexts": [], "citingPaper": {"paperId": "6d951d939d3f27054215f2606a0cf89ed21550e9", "externalIds": {"ArXiv": "2212.02216", "DBLP": "journals/corr/abs-2212-02216", "DOI": "10.48550/arXiv.2212.02216", "CorpusId": 254246441}, "url": "https://www.semanticscholar.org/paper/6d951d939d3f27054215f2606a0cf89ed21550e9", "title": "Improving Few-Shot Performance of Language Models via Nearest Neighbor Calibration", "abstract": "Pre-trained language models (PLMs) have exhibited remarkable few-shot learning capabilities when provided a few examples in a natural language prompt as demonstrations of test instances, i.e., in-context learning. However, the performance of in-context learning is susceptible to the choice of prompt format, training examples and the ordering of the training examples. In this paper, we propose a novel nearest-neighbor calibration framework for in-context learning to ease this issue. It is inspired by a phenomenon that the in-context learning paradigm produces incorrect labels when inferring training instances, which provides a useful supervised signal to calibrate predictions. Thus, our method directly augments the predictions with a $k$-nearest-neighbor ($k$NN) classifier over a datastore of cached few-shot instance representations obtained by PLMs and their corresponding labels. Then adaptive neighbor selection and feature regularization modules are introduced to make full use of a few support instances to reduce the $k$NN retrieval noise. Experiments on various few-shot text classification tasks demonstrate that our method significantly improves in-context learning, while even achieving comparable performance with state-of-the-art tuning-based approaches in some sentiment analysis tasks.", "venue": "arXiv.org", "year": 2022, "citationCount": 7, "influentialCitationCount": 0, "authors": [{"authorId": "7645598", "name": "Feng Nie"}, {"authorId": "2133596091", "name": "Meixi Chen"}, {"authorId": "4947404", "name": "Zhirui Zhang"}, {"authorId": "2110251551", "name": "Xuan Cheng"}]}}, {"contexts": [], "citingPaper": {"paperId": "d19bae780d5fe93e4d007e325c278598ec7f9ea4", "externalIds": {"ArXiv": "2212.01853", "DBLP": "journals/corr/abs-2212-01853", "DOI": "10.48550/arXiv.2212.01853", "CorpusId": 254246784}, "url": "https://www.semanticscholar.org/paper/d19bae780d5fe93e4d007e325c278598ec7f9ea4", "title": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE", "abstract": "This technical report briefly describes our JDExplore d-team's Vega v2 submission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the widely used general language understanding evaluation (GLUE) benchmark, containing eight difficult language understanding tasks, including question answering, natural language inference, word sense disambiguation, coreference resolution, and reasoning. [Method] Instead of arbitrarily increasing the size of a pretrained language model (PLM), our aim is to 1) fully extract knowledge from the input pretraining data given a certain parameter budget, e.g., 6B, and 2) effectively transfer this knowledge to downstream tasks. To achieve goal 1), we propose self-evolution learning for PLMs to wisely predict the informative tokens that should be masked, and supervise the masked language modeling (MLM) process with rectified smooth labels. For goal 2), we leverage the prompt transfer technique to improve the low-resource tasks by transferring the knowledge from the foundation model and related downstream tasks to the target task. [Results] According to our submission record (Oct. 2022), with our optimized pretraining and fine-tuning strategies, our 6B Vega method achieved new state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE leaderboard on Oct. 8, 2022, with an average score of 91.3.", "venue": "arXiv.org", "year": 2022, "citationCount": 11, "influentialCitationCount": 0, "authors": [{"authorId": "2114810150", "name": "Qihuang Zhong"}, {"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "1895813", "name": "Yibing Zhan"}, {"authorId": "145858545", "name": "Y. Qiao"}, {"authorId": "2114783695", "name": "Yonggang Wen"}, {"authorId": "2144035454", "name": "Li Shen"}, {"authorId": "46701032", "name": "Juhua Liu"}, {"authorId": "2110431812", "name": "Baosheng Yu"}, {"authorId": "2142452296", "name": "Bo Du"}, {"authorId": "2116664181", "name": "Yixin Chen"}, {"authorId": "10699750", "name": "Xinbo Gao"}, {"authorId": "2158509654", "name": "Chun Miao"}, {"authorId": "2118488277", "name": "Xiaoou Tang"}, {"authorId": "2140448089", "name": "Dacheng Tao"}]}}, {"contexts": [], "citingPaper": {"paperId": "4caf8b3c92108bb17c93cbf69676680cabffb475", "externalIds": {"ACL": "2023.acl-long.46", "ArXiv": "2212.01378", "DBLP": "conf/acl/Don-YehiyaVRSC23", "DOI": "10.48550/arXiv.2212.01378", "CorpusId": 254220858}, "url": "https://www.semanticscholar.org/paper/4caf8b3c92108bb17c93cbf69676680cabffb475", "title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning", "abstract": "Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining.Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. In this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. Consequentially, ColD Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on.We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. We find ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.19 points on average without any changes to the architecture.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 13, "influentialCitationCount": 0, "authors": [{"authorId": "2165660883", "name": "Shachar Don-Yehiya"}, {"authorId": "5598623", "name": "Elad Venezian"}, {"authorId": "2402716", "name": "Colin Raffel"}, {"authorId": "1766595", "name": "N. Slonim"}, {"authorId": "1722434", "name": "Yoav Katz"}, {"authorId": "41019330", "name": "Leshem Choshen"}]}}, {"contexts": ["\u2026has attracted significant interest (Ni and Wang, 2017; Ishiwatari et al., 2019) and has found success in semantic tasks (Huang et al., 2019; Bevilacqua et al., 2020) such as Word Sense Disambiguation (Bevilacqua et al., 2021, WSD) and Word-in-Context (Pilehvar and Camacho-Collados, 2019, WiC).", "Although introduced a few years ago now, Definition Modeling has attracted significant interest (Ni and Wang, 2017; Ishiwatari et al., 2019) and has found success in semantic tasks (Huang et al., 2019; Bevilacqua et al., 2020) such as Word Sense Disambiguation (Bevilacqua et al., 2021, WSD) and Word-in-Context (Pilehvar and Camacho-Collados, 2019, WiC)."], "citingPaper": {"paperId": "8f75fe2a7b32f241f0d2c7a3a811b304c643e183", "externalIds": {"DBLP": "conf/emnlp/ConiaBSN22", "ArXiv": "2212.01094", "DOI": "10.48550/arXiv.2212.01094", "CorpusId": 254220857}, "url": "https://www.semanticscholar.org/paper/8f75fe2a7b32f241f0d2c7a3a811b304c643e183", "title": "Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures", "abstract": "One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments. However, we argue this need not be the case. In this paper, we present an approach that leverages Definition Modeling to introduce a generalized formulation of SRL as the task of describing predicate-argument structures using natural language definitions instead of discrete labels. Our novel formulation takes a first step towards placing interpretability and flexibility foremost, and yet our experiments and analyses on PropBank-style and FrameNet-style, dependency-based and span-based SRL also demonstrate that a flexible model with an interpretable output does not necessarily come at the expense of performance. We release our software for research purposes at https://github.com/SapienzaNLP/dsrl.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 2, "influentialCitationCount": 0, "authors": [{"authorId": "1396456007", "name": "Simone Conia"}, {"authorId": "1810690342", "name": "Edoardo Barba"}, {"authorId": "11386282", "name": "Alessandro Scir\u00e9"}, {"authorId": "1733928", "name": "Roberto Navigli"}]}}, {"contexts": [], "citingPaper": {"paperId": "5eba60c0c5e5f1f0c6c652dfd48b07caeebf13f4", "externalIds": {"DBLP": "journals/corr/abs-2212-00196", "ArXiv": "2212.00196", "DOI": "10.48550/arXiv.2212.00196", "CorpusId": 254125437}, "url": "https://www.semanticscholar.org/paper/5eba60c0c5e5f1f0c6c652dfd48b07caeebf13f4", "title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors", "abstract": "Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current practice of finetuning models on uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of finetuning on cross-task nearest neighbors is significantly more data-efficient. Using only 2% of the data from the P3 pool without any labeled target-task data, our models outperform strong baselines trained on all available data by 3-30% on 12 out of 14 datasets representing held-out tasks including legal and scientific document QA. Similarly, models trained on cross-task nearest neighbors from SuperNaturalInstructions, representing about 5% of the pool, obtain comparable performance to state-of-the-art models on 12 held-out tasks from that pool. Moreover, the models produced by our approach also provide a better initialization than single multitask finetuned models for few-shot finetuning on target-task data, as shown by a 2-23% relative improvement over few-shot finetuned T0-3B models on 8 datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 4, "influentialCitationCount": 2, "authors": [{"authorId": "2056776606", "name": "Hamish Ivison"}, {"authorId": "1685669", "name": "Noah A. Smith"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}, {"authorId": "2697425", "name": "Pradeep Dasigi"}]}}, {"contexts": [], "citingPaper": {"paperId": "d6ade4de1c5bccb052eb20e6d65f5232c8b61dc5", "externalIds": {"ArXiv": "2211.09113", "DBLP": "conf/emnlp/ZhaoMM22", "ACL": "2022.emnlp-main.262", "DOI": "10.48550/arXiv.2211.09113", "CorpusId": 253475828}, "url": "https://www.semanticscholar.org/paper/d6ade4de1c5bccb052eb20e6d65f5232c8b61dc5", "title": "On Measuring the Intrinsic Few-Shot Hardness of Datasets", "abstract": "While advances in pre-training have led to dramatic improvements in few-shot learning of NLP tasks, there is limited understanding of what drives successful few-shot adaptation in datasets. In particular, given a new dataset and a pre-trained model, what properties of the dataset make it few-shot learnable, and are these properties independent of the specific adaptation techniques used? We consider an extensive set of recent few-shot learning methods and show that their performance across a large number of datasets is highly correlated, showing that few-shot hardness may be intrinsic to datasets, for a given pre-trained model. To estimate intrinsic few-shot hardness, we then propose a simple and lightweight metric called Spread that captures the intuition that few-shot learning is made possible by exploiting feature-space invariances between training and test samples. Our metric better accounts for few-shot hardness compared to existing notions of hardness and is ~8-100x faster to compute.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "1500662261", "name": "Xinran Zhao"}, {"authorId": "37824171", "name": "Shikhar Murty"}, {"authorId": "144783904", "name": "Christopher D. Manning"}]}}, {"contexts": ["\u20262020)), coreference resolution (WSC (Levesque et al., 2012), Winogrande (Sakaguchi et al., 2020)), sentence completion (COPA (Roemmele et al., 2011), StoryCloze (Mostafazadeh et al., 2017), Hellaswag (Zellers et al., 2019)), and word sense disambiguation (WiC (Pilehvar & Camacho-Collados, 2019))."], "citingPaper": {"paperId": "5de4860323ffaba9b7f5aefeedc2d8db2a529a96", "externalIds": {"DBLP": "journals/corr/abs-2211-04668", "ArXiv": "2211.04668", "DOI": "10.48550/arXiv.2211.04668", "CorpusId": 253420466}, "url": "https://www.semanticscholar.org/paper/5de4860323ffaba9b7f5aefeedc2d8db2a529a96", "title": "Zero-Label Prompt Selection", "abstract": "Natural language prompts have been shown to facilitate cross-task generalization for large language models. However, with no or limited labeled examples, the cross-task performance is highly sensitive to the choice of prompts, while selecting a high-performing prompt is challenging given the scarcity of labels. To address the issue, we propose a Zero-Label Prompt Selection (ZPS) method that selects prompts without any labeled data or gradient update. Specifically, given the candidate human-written prompts for a task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the pseudo-labels for prompt selection. Experiments show that ZPS improves over prior methods by a sizeable margin in zero-label performance. We also extend ZPS to a few-shot setting and show its advantages over strong baselines such as prompt tuning and model tuning.", "venue": "arXiv.org", "year": 2022, "citationCount": 2, "influentialCitationCount": 1, "authors": [{"authorId": "2150047631", "name": "Chonghua Liao"}, {"authorId": "2111090780", "name": "Yanan Zheng"}, {"authorId": "2109512754", "name": "Zhilin Yang"}]}}, {"contexts": ["We just introduce the WiC dataset(Pilehvar and Camacho-Collados, 2018) for data augmentation in this paper."], "citingPaper": {"paperId": "bbd1c0d3b11947447a6e1f1e869e98344de86409", "externalIds": {"ArXiv": "2211.03466", "ACL": "2022.evonlp-1.2", "DBLP": "journals/corr/abs-2211-03466", "DOI": "10.48550/arXiv.2211.03466", "CorpusId": 253384279}, "url": "https://www.semanticscholar.org/paper/bbd1c0d3b11947447a6e1f1e869e98344de86409", "title": "Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC", "abstract": "This paper mainly describes the dma submission to the TempoWiC task, which achieves a macro-F1 score of 77.05% and attains the first place in this task. We first explore the impact of different pre-trained language models. Then we adopt data cleaning, data augmentation, and adversarial training strategies to enhance the model generalization and robustness. For further improvement, we integrate POS information and word semantic representation using a Mixture-of-Experts (MoE) approach. The experimental results show that MoE can overcome the feature overuse issue and combine the context, POS, and word semantic features well. Additionally, we use a model ensemble method for the final prediction, which has been proven effective by many research works.", "venue": "EVONLP", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2175617519", "name": "Ze Chen"}, {"authorId": "15885512", "name": "Kangxu Wang"}, {"authorId": "2169253157", "name": "Zijian Cai"}, {"authorId": "2144011733", "name": "Jiewen Zheng"}, {"authorId": "2175549151", "name": "Jiarong He"}, {"authorId": "2190110106", "name": "Max Gao"}, {"authorId": "49050667", "name": "Jason Zhang"}]}}, {"contexts": [], "citingPaper": {"paperId": "1b76caf03527cab16052db606455d9827a4883e3", "externalIds": {"ArXiv": "2211.00107", "DBLP": "journals/corr/abs-2211-00107", "DOI": "10.48550/arXiv.2211.00107", "CorpusId": 253244554}, "url": "https://www.semanticscholar.org/paper/1b76caf03527cab16052db606455d9827a4883e3", "title": "Where to start? Analyzing the potential value of intermediate models", "abstract": "Previous studies observed that finetuned models may be better base models than the vanilla pretrained model. Such a model, finetuned on some source dataset, may provide a better starting point for a new finetuning process on a desired target dataset. Here, we perform a systematic analysis of this intertraining scheme, over a wide range of English classification tasks. Surprisingly, our analysis suggests that the potential intertraining gain can be analyzed independently for the target dataset under consideration, and for a base model being considered as a starting point. This is in contrast to current perception that the alignment between the target dataset and the source dataset used to generate the base model is a major factor in determining intertraining success. We analyze different aspects that contribute to each. Furthermore, we leverage our analysis to propose a practical and efficient approach to determine if and how to select a base model in real-world settings. Last, we release an updating ranking of best models in the HuggingFace hub per architecture https://ibm.github.io/model-recycling/.", "venue": "arXiv.org", "year": 2022, "citationCount": 14, "influentialCitationCount": 3, "authors": [{"authorId": "41019330", "name": "Leshem Choshen"}, {"authorId": "5598623", "name": "Elad Venezian"}, {"authorId": "2165660883", "name": "Shachar Don-Yehiya"}, {"authorId": "1766595", "name": "N. Slonim"}, {"authorId": "1722434", "name": "Yoav Katz"}]}}, {"contexts": [], "citingPaper": {"paperId": "7ffb3a27a2a4da5c35472bd3a3a4dee8d40a6d86", "externalIds": {"DBLP": "journals/corr/abs-2210-16433", "ArXiv": "2210.16433", "DOI": "10.48550/arXiv.2210.16433", "CorpusId": 253238033}, "url": "https://www.semanticscholar.org/paper/7ffb3a27a2a4da5c35472bd3a3a4dee8d40a6d86", "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models", "abstract": "Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC_Large with 770M parameters easily outperforms large language models (LMs) that are 4-39x larger by a large margin. We also demonstrate that KiC exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 8, "influentialCitationCount": 0, "authors": [{"authorId": "34741133", "name": "Xiaoman Pan"}, {"authorId": "2087264100", "name": "Wenlin Yao"}, {"authorId": "49723569", "name": "Hongming Zhang"}, {"authorId": "41190054", "name": "Dian Yu"}, {"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "2108276402", "name": "Jianshu Chen"}]}}, {"contexts": [], "citingPaper": {"paperId": "d9bba73140122341c3508511e3f4e14fb872fd2e", "externalIds": {"ArXiv": "2210.14815", "CorpusId": 253116855}, "url": "https://www.semanticscholar.org/paper/d9bba73140122341c3508511e3f4e14fb872fd2e", "title": "On the Curious Case of $\\ell_2$ norm of Sense Embeddings", "abstract": "We show that the $\\ell_2$ norm of a static sense embedding encodes information related to the frequency of that sense in the training corpus used to learn the sense embeddings. This finding can be seen as an extension of a previously known relationship for word embeddings to sense embeddings. Our experimental results show that, in spite of its simplicity, the $\\ell_2$ norm of sense embeddings is a surprisingly effective feature for several word sense related tasks such as (a) most frequent sense prediction, (b) Word-in-Context (WiC), and (c) Word Sense Disambiguation (WSD). In particular, by simply including the $\\ell_2$ norm of a sense embedding as a feature in a classifier, we show that we can improve WiC and WSD methods that use static sense embeddings.", "venue": "", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "32066669", "name": "Yi Zhou"}, {"authorId": "2075356592", "name": "D. Bollegala"}]}}, {"contexts": [", 2001) classification Word-in-Context (Pilehvar and Camacho-Collados, 2018) Sentiment Classification Yelp Polarity (Zhang et al."], "citingPaper": {"paperId": "7633c8b7cd4209222d02c453184b480533a0e139", "externalIds": {"DBLP": "journals/corr/abs-2210-14606", "ArXiv": "2210.14606", "ACL": "2022.gem-1.5", "DOI": "10.48550/arXiv.2210.14606", "CorpusId": 253117139}, "url": "https://www.semanticscholar.org/paper/7633c8b7cd4209222d02c453184b480533a0e139", "title": "Analyzing Multi-Task Learning for Abstractive Text Summarization", "abstract": "Despite the recent success of multi-task learning and pre-finetuning for natural language understanding, few works have studied the effects of task families on abstractive text summarization. Task families are a form of task grouping during the pre-finetuning stage to learn common skills, such as reading comprehension. To close this gap, we analyze the influence of multi-task learning strategies using task families for the English abstractive text summarization task. We group tasks into one of three strategies, i.e., sequential, simultaneous, and continual multi-task learning, and evaluate trained models through two downstream tasks. We find that certain combinations of task families (e.g., advanced reading comprehension and natural language inference) positively impact downstream performance. Further, we find that choice and combinations of task families influence downstream performance more than the training scheme, supporting the use of task families for abstractive text", "venue": "IEEE Games Entertainment Media Conference", "year": 2022, "citationCount": 3, "influentialCitationCount": 1, "authors": [{"authorId": "2187206992", "name": "Frederic Kirstein"}, {"authorId": "2056772135", "name": "Jan Philip Wahle"}, {"authorId": "8837621", "name": "Terry Ruas"}, {"authorId": "145151838", "name": "Bela Gipp"}]}}, {"contexts": [], "citingPaper": {"paperId": "d130837a5601f4f82380736897d07f3008de1fa4", "externalIds": {"ArXiv": "2210.13311", "DBLP": "journals/corr/abs-2210-13311", "DOI": "10.48550/arXiv.2210.13311", "CorpusId": 253098398}, "url": "https://www.semanticscholar.org/paper/d130837a5601f4f82380736897d07f3008de1fa4", "title": "Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning", "abstract": "Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the new paradigm for using pre-trained language models (PLMs). Up to now, various DETs with distinct design elements have been proposed, achieving performance on par with fine-tuning. However, the mechanisms behind the above success are still under-explored, especially the connections among various DETs. To fathom the mystery, we hypothesize that the adaptations of different DETs could all be reparameterized as low-dimensional optimizations in a unified optimization subspace, which could be found by jointly decomposing independent solutions of different DETs. Then we explore the connections among different DETs by conducting optimization within the subspace. In experiments, we find that, for a certain DET, conducting optimization simply in the subspace could achieve comparable performance to its original space, and the found solution in the subspace could be transferred to another DET and achieve non-trivial performance. We also visualize the performance landscape of the subspace and find that there exists a substantial region where different DETs all perform well. Finally, we extend our analysis and show the strong connections between fine-tuning and DETs.", "venue": "arXiv.org", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "2106388389", "name": "Jing Yi"}, {"authorId": "2109136284", "name": "Weize Chen"}, {"authorId": "50625437", "name": "Yujia Qin"}, {"authorId": "2149202150", "name": "Yankai Lin"}, {"authorId": "46649145", "name": "Ning Ding"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "2141313179", "name": "Zhiyuan Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "49178343", "name": "Jie Zhou"}]}}, {"contexts": ["\u20262018) and SuperGLUE (Wang et al., 2019) tasks: WNLI (Wang et al., 2018), MRPC (Dolan et al., 2005), BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), RTE (Giampiccolo et al., 2007), WiC (Pilehvar & Camacho-Collados, 2019), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019).", "We evaluate our model on the following GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) tasks: WNLI (Wang et al., 2018), MRPC (Dolan et al., 2005), BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), RTE (Giampiccolo et al., 2007), WiC (Pilehvar & Camacho-Collados, 2019), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019).", "Pearson product-moment correlation coefficients between attention weights and F1 scores are shown as follows,\nTarget Task WNLI MRPC RTE MultiRC BoolQ WiC WSC CB Corr. coeff.", "We then evaluate our model on the following GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) tasks: WNLI (Wang et al., 2018), MRPC (Dolan et al., 2005), BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), RTE (Giampiccolo et al., 2007), WiC (Pilehvar & CamachoCollados, 2019), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019)."], "citingPaper": {"paperId": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899", "externalIds": {"DBLP": "journals/corr/abs-2210-12587", "ArXiv": "2210.12587", "DOI": "10.48550/arXiv.2210.12587", "CorpusId": 253098972}, "url": "https://www.semanticscholar.org/paper/3d7d385d9ee75a286e8da27f7d3cf9f12651c899", "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning", "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 4, "influentialCitationCount": 0, "authors": [{"authorId": "2115814529", "name": "Xiangyu Peng"}, {"authorId": "50461046", "name": "Chen Xing"}, {"authorId": "3466801", "name": "Prafulla Kumar Choubey"}, {"authorId": "30340989", "name": "Chien-Sheng Wu"}, {"authorId": "2054594326", "name": "Caiming Xiong"}]}}, {"contexts": [], "citingPaper": {"paperId": "71f305a5779b791339f13725a362fd339ee7f3ba", "externalIds": {"DBLP": "conf/emnlp/WhitehouseCI22", "ArXiv": "2210.12540", "DOI": "10.48550/arXiv.2210.12540", "CorpusId": 253098059}, "url": "https://www.semanticscholar.org/paper/71f305a5779b791339f13725a362fd339ee7f3ba", "title": "EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching", "abstract": "Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at the word- or phrase-level, in contrast to sentence-level via parallel instances. Existing approaches either use dictionaries or parallel sentences with word alignment to generate CS data by randomly switching words in a sentence. However, such methods can be suboptimal as dictionaries disregard semantics, and syntax might become invalid after random word switching. In this work, we propose EntityCS, a method that focuses on Entity-level Code-Switching to capture fine-grained cross-lingual semantics without corrupting syntax. We use Wikidata and English Wikipedia to construct an entity-centric CS corpus by switching entities to their counterparts in other languages. We further propose entity-oriented masking strategies during intermediate model training on the EntityCS corpus for improving entity prediction. Evaluation of the trained models on four entity-centric downstream tasks shows consistent improvements over the baseline with a notable increase of 10% in Fact Retrieval. We release the corpus and models to assist research on code-switching and enriching XLMs with external knowledge.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 3, "influentialCitationCount": 0, "authors": [{"authorId": "2161240241", "name": "Chenxi Whitehouse"}, {"authorId": "48810605", "name": "Fenia Christopoulou"}, {"authorId": "2676143", "name": "Ignacio Iacobacci"}]}}, {"contexts": ["WordNet is a popular lexical resource for NLP, but its senses for words can be overly finegrained (Pilehvar and Camacho-Collados, 2019) and not suitable for all domains."], "citingPaper": {"paperId": "7f5801f8036e71658a12d16203617b2ab25ef14f", "externalIds": {"ArXiv": "2210.12170", "DBLP": "journals/corr/abs-2210-12170", "ACL": "2022.emnlp-main.228", "DOI": "10.48550/arXiv.2210.12170", "CorpusId": 253098158}, "url": "https://www.semanticscholar.org/paper/7f5801f8036e71658a12d16203617b2ab25ef14f", "title": "Discovering Differences in the Representation of People using Contextualized Semantic Axes", "abstract": "A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against \u201csemantic axes\u201d that represent two opposing concepts. We extend this paradigm to BERT embeddings, and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations. We validate and demonstrate these axes on two people-centric datasets: occupations from Wikipedia, and multi-platform discussions in extremist, men\u2019s communities over fourteen years. In both studies, contextualized semantic axes can characterize differences among instances of the same word type. In the latter study, we show that references to women and the contexts around them have become more detestable over time.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 3, "influentialCitationCount": 1, "authors": [{"authorId": "15983089", "name": "Li Lucy"}, {"authorId": "2174240589", "name": "Divya Tadimeti"}, {"authorId": "2064411219", "name": "David Bamman"}]}}, {"contexts": [], "citingPaper": {"paperId": "82cd40e926300b6b18c34ced2edeb07e84d9d6c7", "externalIds": {"DBLP": "conf/emnlp/GuK0H22", "ACL": "2022.emnlp-main.105", "ArXiv": "2210.09175", "DOI": "10.48550/arXiv.2210.09175", "CorpusId": 252918165}, "url": "https://www.semanticscholar.org/paper/82cd40e926300b6b18c34ced2edeb07e84d9d6c7", "title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization", "abstract": "Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft instructions, has been shown effective in instruction learning for unseen tasks. However, IT relies on a large amount of human-annotated samples, which restricts its generalization. Unlike labeled data, unlabeled data are often massive and cheap to obtain. In this work, we study how IT can be improved with unlabeled data. We first empirically explore the IT performance trends versus the number of labeled data, instructions, and training tasks. We find it critical to enlarge the number of training instructions, and the instructions can be underutilized due to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts. We conduct extensive experiments to show UDIT\u2019s effectiveness in various scenarios of tasks and datasets. We also comprehensively analyze the key factors of UDIT to investigate how to better improve IT with unlabeled data. The code is publicly available at https://github.com/thu-coai/UDIT.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 7, "influentialCitationCount": 1, "authors": [{"authorId": "2116405624", "name": "Yuxian Gu"}, {"authorId": "1886879", "name": "Pei Ke"}, {"authorId": "145213540", "name": "Xiaoyan Zhu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}}, {"contexts": ["Word-in-Context (Pilehvar and CamachoCollados, 2019) is a word sense disambiguation task that aims to predict whether the word is used with the same sense in sentence pairs."], "citingPaper": {"paperId": "15292ec8582514311e4460629361d0ec76882343", "externalIds": {"DBLP": "conf/emnlp/Zhong00ML0T22", "ArXiv": "2210.05497", "DOI": "10.48550/arXiv.2210.05497", "CorpusId": 252815713}, "url": "https://www.semanticscholar.org/paper/15292ec8582514311e4460629361d0ec76882343", "title": "Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models", "abstract": "Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However, SAM adds a perturbation to each model parameter equally (but not all parameters contribute equally to the optimization of training), which we argue is sub-optimal and will lead to excessive computation. In this paper, we propose a novel optimization procedure, namely FSAM, which introduces a Fisher mask to improve the efficiency and performance of SAM. In short, instead of adding perturbation to all parameters, FSAM uses the Fisher information to identity the important parameters and formulates a Fisher mask to obtain the sparse perturbation, i.e., making the optimizer focus on these important parameters. Experiments on various tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently outperforms the vanilla SAM by 0.67~1.98 average score among four different pretrained models. We also empirically show that FSAM works well in other complex scenarios, e.g., fine-tuning on generation tasks or limited training data. Encouragingly, when training data is limited, FSAM improves the SAM by a large margin, i.e., up to 15.1.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 24, "influentialCitationCount": 0, "authors": [{"authorId": "2114810150", "name": "Qihuang Zhong"}, {"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "2144035454", "name": "Li Shen"}, {"authorId": "2148392043", "name": "Peng Mi"}, {"authorId": "46701032", "name": "Juhua Liu"}, {"authorId": "2142452296", "name": "Bo Du"}, {"authorId": "2140448089", "name": "Dacheng Tao"}]}}, {"contexts": ["The SuperGLUE tasks include BoolQ (Clark et al., 2019a), CB (De Marneffe et al., 2019), COPA (Roemmele et al., 2011), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE, WiC (Pilehvar and Camacho-Collados, 2019), and WSC (Levesque et al., 2012)."], "citingPaper": {"paperId": "abdd13184f276533dfe4fd7ffb6d99b1b762a43d", "externalIds": {"DBLP": "journals/corr/abs-2210-05043", "ArXiv": "2210.05043", "ACL": "2023.acl-long.48", "DOI": "10.48550/arXiv.2210.05043", "CorpusId": 252816084}, "url": "https://www.semanticscholar.org/paper/abdd13184f276533dfe4fd7ffb6d99b1b762a43d", "title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling", "abstract": "Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need only fine-tune our single Multi-CLS BERT model (and run the one model at test time, ensembling just the multiple final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS BERT reliably improves both overall accuracy and confidence estimation. When only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model can even outperform the corresponding BERT_Large model. We analyze the behavior of our Multi-CLS BERT, showing that it has many of the same characteristics and behavior as a typical BERT 5-way ensemble, but with nearly 4-times less computation and memory.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "144827671", "name": "Haw-Shiuan Chang"}, {"authorId": "2146872867", "name": "Ruei-Yao Sun"}, {"authorId": "2094934507", "name": "Kathryn Ricci"}, {"authorId": "143753639", "name": "A. McCallum"}]}}, {"contexts": [], "citingPaper": {"paperId": "40e49ba41eca31f9c2661cc65f2c13dc4f2c7859", "externalIds": {"DBLP": "journals/corr/abs-2210-04457", "ACL": "2022.emnlp-main.758", "ArXiv": "2210.04457", "DOI": "10.48550/arXiv.2210.04457", "CorpusId": 252780166}, "url": "https://www.semanticscholar.org/paper/40e49ba41eca31f9c2661cc65f2c13dc4f2c7859", "title": "XPrompt: Exploring the Extreme of Prompt Tuning", "abstract": "Prompt tuning learns soft prompts to condition the frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large performance gap between prompt tuning and fine-tuning for models of moderate and small scales (typically less than 11B parameters). In this paper, we empirically show that the trained prompt tokens can have a negative impact on a downstream task and thus degrade its performance. To bridge the gap, we propose a novel Prompt tuning model with an eXtremely small scale (XPrompt) under the regime of lottery tickets hypothesis. Specifically, XPrompt eliminates the negative prompt tokens at different granularity levels through a hierarchical structured pruning, yielding a more parameter-efficient prompt yet with a competitive performance. Comprehensive experiments are carried out on the SuperGLUE tasks, and the results indicate that XPrompt is able to close the performance gap at smaller model scales.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 7, "influentialCitationCount": 0, "authors": [{"authorId": "2163587600", "name": "Fang Ma"}, {"authorId": "145107889", "name": "Chen Zhang"}, {"authorId": "2152318755", "name": "Lei Ren"}, {"authorId": "2109593338", "name": "Jingang Wang"}, {"authorId": "2145778781", "name": "Qifan Wang"}, {"authorId": "50224935", "name": "Wei Yu Wu"}, {"authorId": "38472218", "name": "Xiaojun Quan"}, {"authorId": "2151679983", "name": "Dawei Song"}]}}, {"contexts": ["It is thus unclear whether the WiC task requires true reasoning that benefits from the depiction of intermediate reasoning steps.", "In this section, we present the details of the chain-of-thought prompts used in our paper for the XCOPA (Figure 9) and the XL-WiC (Figures 10 and 11) tasks.", "To better understand the multilingual reasoning abilities of large pretrained language models, we extend our experiments to two additional multilingual reasoning benchmarks, XCOPA (Ponti et al., 2020) and XL-WiC (Raganato et al., 2020).", "XL-WiC is a multilingual word in-context semantic judgment benchmark covering thirteen languages:5 Bulgarian (BG), Danish (DA), German (DE), Estonian (ET), Persian (FA), French (FR), Croatian (HR), Italian (IT), Japanese (JA), Korean (KO), Dutch (NL) and Chinese (ZH).", "Despite its simplicity, this task is extremely challenging; PaLM-540B only achieves a score of 64.6 on WiC (Pilehvar & Camacho-Collados, 2019), the English version of the task."], "citingPaper": {"paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "externalIds": {"DBLP": "journals/corr/abs-2210-03057", "ArXiv": "2210.03057", "DOI": "10.48550/arXiv.2210.03057", "CorpusId": 252735112}, "url": "https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "title": "Language Models are Multilingual Chain-of-Thought Reasoners", "abstract": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 72, "influentialCitationCount": 11, "authors": [{"authorId": "8815141", "name": "Freda Shi"}, {"authorId": "51903517", "name": "Mirac Suzgun"}, {"authorId": "35307070", "name": "Markus Freitag"}, {"authorId": "1524732527", "name": "Xuezhi Wang"}, {"authorId": "2187059571", "name": "Suraj Srivats"}, {"authorId": "1918441", "name": "Soroush Vosoughi"}, {"authorId": "3351938", "name": "Hyung Won Chung"}, {"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "2124014463", "name": "Sebastian Ruder"}, {"authorId": "65855107", "name": "Denny Zhou"}, {"authorId": "143790066", "name": "Dipanjan Das"}, {"authorId": "119640649", "name": "Jason Wei"}]}}, {"contexts": ["For analysis of label generalization of classification tasks, we evaluate on 5 datasets: 2 seen datasets during meta-training (IMDB, PAWS) and 3 unseen datasets (RTE, CB, WiC).", "Table 4 shows the label pair variation of binary classification datasets (RTE, WiC, IMDB, PAWS) while Table 5 shows the label pair variation of CB, which consists of 3 label options.", "For RTE, WSC, WiC datasets, which are consisted of seen label options (yes/no), direct meta-trained LMs (DIRECT, T0) show strong performance as shown in Table 2.", "For English NLP tasks, in addition to 11 unseen evaluation datasets from Sanh et al. (2021), we add 3 unseen question-answering datasets from Lin et al. (2022), resulting in 7 classification (RTE (Dagan et al., 2005), CB(De Marneffe et al., 2019), ANLI R1,R2,R3 (Nie et al., 2020) WSC (Levesque et al., 2012), WiC (Pilehvar & Camacho-Collados, 2019)) and 7 multi-choice datasets (COPA (Roemmele et al., 2011), Hellaswag (Zellers et al., 2019), Storycloze (Mostafazadeh et al., 2016), PIQA (Bisk et al., 2020), ARC-Challenge (Clark et al., 2018), OpenbookQA (Mihaylov et al., 2018)).", "\u2026in 7 classification (RTE (Dagan et al., 2005), CB(De Marneffe et al., 2019), ANLI R1,R2,R3 (Nie et al., 2020) WSC (Levesque et al., 2012), WiC (Pilehvar & Camacho-Collados, 2019)) and 7 multi-choice datasets (COPA (Roemmele et al., 2011), Hellaswag (Zellers et al., 2019), Storycloze\u2026"], "citingPaper": {"paperId": "07ec0d4cc6a2be39def51139d228292c6a0dc627", "externalIds": {"DBLP": "conf/iclr/YeKJSS23", "ArXiv": "2210.02969", "DOI": "10.48550/arXiv.2210.02969", "CorpusId": 252735240}, "url": "https://www.semanticscholar.org/paper/07ec0d4cc6a2be39def51139d228292c6a0dc627", "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners", "abstract": "Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4% and 9.7% points, respectively. Flipped gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% average F1 score. This indicates that the strong task generalization of Flipped comes from improved generalization to novel labels. We release our code at https://github.com/seonghyeonye/Flipped-Learning.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 8, "influentialCitationCount": 1, "authors": [{"authorId": "2152111477", "name": "Seonghyeon Ye"}, {"authorId": "2180527259", "name": "Doyoung Kim"}, {"authorId": "2000091730", "name": "Joel Jang"}, {"authorId": "27582486", "name": "Joongbo Shin"}, {"authorId": "4418074", "name": "Minjoon Seo"}]}}, {"contexts": ["Furthermore, while T0 outperforms GPT-3 on 3 datasets (RTE, StoryCloze, WiC), T0+ROSPR additionally outperforms GPT-3 on 2 datasets (ANLI R1 and CB) and enlarging the score gap for RTE, StoryCloze and WiC.", "\u2026et al., 2011), Hellaswag (Zellers et al., 2019), Storycloze (Mostafazadeh et al., 2016) for sentence completion task, Winogrande (Sakaguchi et al., 2021), WSC (Levesque et al., 2012) for coreference resolution task, and WiC (Pilehvar and Camacho-Collados, 2019) for word sense disambiguation task.", "Same as Section 7, we report the mean accuracy of 4 evaluation datasets: RTE, COPA, Winogrande, and WiC with 3 runs with different random seeds for the sampling of evaluation queries.", "We additionally analyze the effect of answer choice formats on RTE and WiC datasets by retrieving prompt embeddings trained on various source tasks.", "Counterintuitively, for both RTE and WiC target tasks, when the answer choice format is aligned, the task source embedding of sentiment classification, which is known to be irrelevant to RTE and WiC (Pruksachatkun et al., 2020), outperforms other embeddings sourced from datasets that are more relevant to the target datasets (paraphrase and multi-choice QA) (Appendix E).", "We experiment on 3 datasets (RTE, COPA, WiC) which correspond to different tasks (NLI, sentence completion, word sense disambiguation) respectively.", "F.2 Evaluation Datasets\nFollowing Sanh et al. (2021), we include 11 evaluation datasets as follows: RTE (Dagan et al., 2005), CB (De Marneffe et al., 2019), ANLI (Nie et al., 2020) for natural language inference task, COPA (Roemmele et al., 2011), Hellaswag (Zellers et al., 2019), Storycloze (Mostafazadeh et al., 2016) for sentence completion task, Winogrande (Sakaguchi et al., 2021), WSC (Levesque et al., 2012) for coreference resolution task, and WiC (Pilehvar and Camacho-Collados, 2019) for word sense disambiguation task.", "We evaluate variations of our proposed methods on 4 datasets: RTE for NLI, COPA for sentence completion, Winogrande\nfor coreference, and WiC for word sense disambiguation task."], "citingPaper": {"paperId": "2303ee0de927266c296287202519f17bdea9e4e8", "externalIds": {"DBLP": "journals/corr/abs-2210-03029", "ArXiv": "2210.03029", "DOI": "10.48550/arXiv.2210.03029", "CorpusId": 252734741}, "url": "https://www.semanticscholar.org/paper/2303ee0de927266c296287202519f17bdea9e4e8", "title": "Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization", "abstract": "During zero-shot inference with language models (LMs), using hard prompts alone may not be able to fully describe the target task. In this paper, we explore how the retrieval of soft prompts obtained through prompt tuning can assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances (hard prompt + input instances) mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. Results show this simple approach enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points while adding only 0.007% additional parameters. Also, using interpolation of multiple embeddings and variance-based ranking further improve accuracy and robustness to different evaluation prompts, widening the performance gap. Finally, we find that retrieving source embeddings trained on similar answer choice formats is more important than those on similar task types. Model checkpoints and code implementation are available at https://github.com/seonghyeonye/RoSPr.", "venue": "arXiv.org", "year": 2022, "citationCount": 8, "influentialCitationCount": 0, "authors": [{"authorId": "2152111477", "name": "Seonghyeon Ye"}, {"authorId": "2000091730", "name": "Joel Jang"}, {"authorId": "2180527259", "name": "Doyoung Kim"}, {"authorId": "2144061605", "name": "Yongrae Jo"}, {"authorId": "4418074", "name": "Minjoon Seo"}]}}, {"contexts": ["As this setup does not focus on generalisation per se, but rather depends on the quality of the translation model, we will not further discuss it.\nand multilingual WordNet and Wiktionary have been used to build XL-WiC (Raganato et al., 2020), an extension of WiC (Pilehvar and Camacho-Collados, 2019) that reformulates word sense disambiguation in 12 languages as a binary classification task.", ", 2020), an extension of WiC (Pilehvar and Camacho-Collados, 2019) that reformulates word sense disambiguation in 12 languages as a binary classification task."], "citingPaper": {"paperId": "559bfba3bee31f6061a5d5c7061f22794de47e39", "externalIds": {"ArXiv": "2210.03050", "DBLP": "journals/corr/abs-2210-03050", "DOI": "10.48550/arXiv.2210.03050", "CorpusId": 252735124}, "url": "https://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39", "title": "State-of-the-art generalisation research in NLP: a taxonomy and review", "abstract": "The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what \u2018good generalisation\u2019 entails and how it should be evaluated is not well understood, nor are there any common standards to evaluate it. In this paper, we aim to lay the groundwork to improve both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP, we use that taxonomy to present a comprehensive map of published generalisation studies, and we make recommendations for which areas might deserve attention in the future. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they aim to solve, the type of data shift they consider, the source by which this data shift is obtained, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 previous papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis of the current state of generalisation research in NLP, and make recommendations for the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to make steps towards making state-of-the-art generalisation testing the new status quo in NLP.", "venue": "arXiv.org", "year": 2022, "citationCount": 24, "influentialCitationCount": 2, "authors": [{"authorId": "3449411", "name": "D. Hupkes"}, {"authorId": "24068173", "name": "Mario Giulianelli"}, {"authorId": "74461595", "name": "Verna Dankers"}, {"authorId": "2347956", "name": "Mikel Artetxe"}, {"authorId": "51131518", "name": "Yanai Elazar"}, {"authorId": "1388571351", "name": "Tiago Pimentel"}, {"authorId": "2718039", "name": "Christos Christodoulopoulos"}, {"authorId": "51121444", "name": "Karim Lasri"}, {"authorId": "2362960", "name": "Naomi Saphra"}, {"authorId": "153915609", "name": "Arabella J. Sinclair"}, {"authorId": "133864309", "name": "Dennis Ulmer"}, {"authorId": "2187058204", "name": "Florian Schottmann"}, {"authorId": "2049136", "name": "Khuyagbaatar Batsuren"}, {"authorId": "2087314342", "name": "Kaiser Sun"}, {"authorId": "40910779", "name": "Koustuv Sinha"}, {"authorId": "50824937", "name": "Leila Khalatbari"}, {"authorId": "1410648718", "name": "Maria Ryskina"}, {"authorId": "1397300094", "name": "Rita Frieske"}, {"authorId": "2070989574", "name": "Ryan Cotterell"}, {"authorId": "2111472502", "name": "Zhijing Jin"}]}}, {"contexts": ["\u2026Marneffe et al., 2019], COPA [Roemmele et al., 2011], MultiRC [Khashabi et al., 2018], ReCoRD [Zhang et al., 2018], RTE [Wang et al., 2019], WiC [Pilehvar and Camacho-Collados, 2018], WSC [Levesque et al., 2012]), NLI (ANLI R1, ANLI R2, ANLI R3 [Nie et al., 2020], StoryCloze [Mostafazadeh et\u2026", "Gold Output\nno\nWiC AMA prompt()-chain Example\nanswer()\nGive synonyms of the word in the sentence.", "Wang et al. [2019] Train Size: 5428, Test Size: 638\nWiC Few Shot Input\nThese aspects of civilization do not find expression or receive an interpretation.", "We evaluate over 20 datasets which fall into 4 categories: SuperGLUE (BoolQ [Clark et al., 2019], CB [De Marneffe et al., 2019], COPA [Roemmele et al., 2011], MultiRC [Khashabi et al., 2018], ReCoRD [Zhang et al., 2018], RTE [Wang et al., 2019], WiC [Pilehvar and Camacho-Collados, 2018], WSC [Levesque et al., 2012]), NLI (ANLI R1, ANLI R2, ANLI R3 [Nie et al., 2020], StoryCloze [Mostafazadeh et al., 2017]), Classification (DBPedia [Zhang et al., 2015], AGNews [Zhang et al., 2015], SST2 [Socher et al., 2013], Amazon [He and McAuley, 2016]), and Question-Answering (RealTimeQA [Kasai et al., 2022], DROP [Dua et al., 2019], Natural Questions [Kwiatkowski et al., 2019], WebQuestions [Berant et al., 2013]).", ", 2019], WiC [Pilehvar and Camacho-Collados, 2018], WSC [Levesque et al."], "citingPaper": {"paperId": "fb49e88c6bd676516898e911e42b4f8479e6f1bf", "externalIds": {"DBLP": "journals/corr/abs-2210-02441", "ArXiv": "2210.02441", "DOI": "10.48550/arXiv.2210.02441", "CorpusId": 252716013}, "url": "https://www.semanticscholar.org/paper/fb49e88c6bd676516898e911e42b4f8479e6f1bf", "title": "Ask Me Anything: A simple strategy for prompting language models", "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly\"perfect prompt\"for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 54, "influentialCitationCount": 4, "authors": [{"authorId": "47038321", "name": "Simran Arora"}, {"authorId": "1381444249", "name": "A. Narayan"}, {"authorId": "48622329", "name": "Mayee F. Chen"}, {"authorId": "4773175", "name": "Laurel J. Orr"}, {"authorId": "2820009", "name": "Neel Guha"}, {"authorId": "2056438239", "name": "Kush S Bhatia"}, {"authorId": "3442125", "name": "Ines Chami"}, {"authorId": "2186982588", "name": "Frederic Sala"}, {"authorId": "2061444681", "name": "Christopher R'e"}]}}, {"contexts": ["For T0 models, there are some tasks such as OBQA and Piqa that are not evaluated in the original paper (Sanh et al., 2021), and some tasks such as CB and WiC are evaluated with slightly different templates.", "For evaluation tasks, we consider seven datasets from five diverse categories following the task taxonomy of T0, including, two sentence completion tasks, COPA (Roemmele et al., 2011) and HellaSwag (HSwag) (Zellers et al., 2019), two multiple-choice QA tasks, OpenbookQA (OBQA) (Mihaylov et al., 2018) and Piqa (Bisk et al., 2020), one word sense disambiguation task, WiC (Pilehvar & Camacho-Collados, 2018), one sentiment task, Rotten Tomatoes (RT) (Pang & Lee, 2005), and one natural language inference task, CB (De Marneffe et al., 2019).", "\u2026two multiple-choice QA tasks, OpenbookQA (OBQA) (Mihaylov et al., 2018) and Piqa (Bisk et al., 2020), one word sense disambiguation task, WiC (Pilehvar & Camacho-Collados, 2018), one sentiment task, Rotten Tomatoes (RT) (Pang & Lee, 2005), and one natural language inference task, CB (De\u2026"], "citingPaper": {"paperId": "ec97c3248537bb0b455b3fe9bc341110cfceffde", "externalIds": {"DBLP": "conf/acl/WangP00CJ23", "ArXiv": "2210.00185", "DOI": "10.48550/arXiv.2210.00185", "CorpusId": 252683285}, "url": "https://www.semanticscholar.org/paper/ec97c3248537bb0b455b3fe9bc341110cfceffde", "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks", "abstract": "Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with an external retriever, have demonstrated promising language modeling capabilities. However, it remains unclear whether such semi-parametric language models can perform competitively well as their fully-parametric counterparts on zero-shot generalization to downstream tasks. In this work, we introduce $\\text{Zemi}$, a zero-shot semi-parametric language model. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train $\\text{Zemi}$ with a novel semi-parametric multitask prompted training paradigm, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, we augment the multitask training and zero-shot evaluation with retrieval from a large-scale task-agnostic unlabeled corpus. In order to incorporate multiple potentially noisy retrieved augmentations, we further propose a novel $\\text{augmentation fusion}$ module leveraging perceiver resampler and gated cross-attention. Notably, our proposed $\\text{Zemi}_\\text{LARGE}$ outperforms T0-3B by 16% on all seven evaluation tasks while being 3.9x smaller in model size.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 4, "influentialCitationCount": 1, "authors": [{"authorId": "2052036545", "name": "Zhenhailong Wang"}, {"authorId": "34741133", "name": "Xiaoman Pan"}, {"authorId": "41190054", "name": "Dian Yu"}, {"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "2108276402", "name": "Jianshu Chen"}, {"authorId": "2113323573", "name": "Heng Ji"}]}}, {"contexts": [], "citingPaper": {"paperId": "848228a5df1c7cec594148230648bd7edccdbd8a", "externalIds": {"DBLP": "conf/coling/YuGZLLSZ22", "ACL": "2022.coling-1.440", "ArXiv": "2209.09401", "DOI": "10.48550/arXiv.2209.09401", "CorpusId": 252383156}, "url": "https://www.semanticscholar.org/paper/848228a5df1c7cec594148230648bd7edccdbd8a", "title": "Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models", "abstract": "Prompting, which casts downstream applications as language modeling tasks, has shown to be sample efficient compared to standard fine-tuning with pre-trained models. However, one pitfall of prompting is the need of manually-designed patterns, whose outcome can be unintuitive and requires large validation sets to tune. To tackle the challenge, we propose AutoSeq, a fully automatic prompting method: (1) We adopt natural language prompts on sequence-to-sequence models, enabling free-form generation and larger label search space; (2) We propose label sequences \u2013 phrases with indefinite lengths to verbalize the labels \u2013 which eliminate the need of manual templates and are more expressive than single label words; (3) We use beam search to automatically generate a large amount of label sequence candidates and propose contrastive re-ranking to get the best combinations. AutoSeq significantly outperforms other no-manual-design methods, such as soft prompt tuning, adapter tuning, and automatic search on single label words; the generated label sequences are even better than curated manual ones on a variety of tasks. Our method reveals the potential of sequence-to-sequence models in few-shot learning and sheds light on a path to generic and automatic prompting. The source code of this paper can be obtained from https://github.com/thunlp/Seq2Seq-Prompt.", "venue": "International Conference on Computational Linguistics", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "authors": [{"authorId": "103985655", "name": "Zichun Yu"}, {"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "2621696", "name": "Zhengyan Zhang"}, {"authorId": "2149202150", "name": "Yankai Lin"}, {"authorId": "2141313179", "name": "Zhiyuan Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "49178343", "name": "Jie Zhou"}]}}, {"contexts": ["TempoWiC follows the simple formulation from the SuperGLUE Word-inContext (WiC) challenge (Pilehvar and CamachoCollados, 2019), which is particularly well suited for temporal meaning shift evaluation given that it is not reliant on a reference sense inventory."], "citingPaper": {"paperId": "bedad2743c6e4804189b7f0b85ec2924a40d7d90", "externalIds": {"DBLP": "journals/corr/abs-2209-07216", "ArXiv": "2209.07216", "ACL": "2022.coling-1.296", "DOI": "10.48550/arXiv.2209.07216", "CorpusId": 252280503}, "url": "https://www.semanticscholar.org/paper/bedad2743c6e4804189b7f0b85ec2924a40d7d90", "title": "TempoWiC: An Evaluation Benchmark for Detecting Meaning Shift in Social Media", "abstract": "Language evolves over time, and word meaning changes accordingly. This is especially true in social media, since its dynamic nature leads to faster semantic shifts, making it challenging for NLP models to deal with new content and trends. However, the number of datasets and models that specifically address the dynamic nature of these social platforms is scarce. To bridge this gap, we present TempoWiC, a new benchmark especially aimed at accelerating research in social media-based meaning shift. Our results show that TempoWiC is a challenging benchmark, even for recently-released language models specialized in social media.", "venue": "International Conference on Computational Linguistics", "year": 2022, "citationCount": 10, "influentialCitationCount": 4, "authors": [{"authorId": "144653901", "name": "Daniel Loureiro"}, {"authorId": "2184924822", "name": "Aminette D'Souza"}, {"authorId": "2184921647", "name": "Areej Nasser Muhajab"}, {"authorId": "2184921830", "name": "Isabella A. White"}, {"authorId": "2184921317", "name": "Gabriel Wong"}, {"authorId": "2254466", "name": "Luis Espinosa Anke"}, {"authorId": "152842060", "name": "Leonardo Neves"}, {"authorId": "1499242010", "name": "Francesco Barbieri"}, {"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}]}}, {"contexts": [], "citingPaper": {"paperId": "aa3bae7def250ba247ea2c187ad1c1a99b3bf737", "externalIds": {"ArXiv": "2208.09669", "DBLP": "journals/corr/abs-2208-09669", "DOI": "10.48550/arXiv.2208.09669", "CorpusId": 251719422}, "url": "https://www.semanticscholar.org/paper/aa3bae7def250ba247ea2c187ad1c1a99b3bf737", "title": "Lost in Context? On the Sense-wise Variance of Contextualized Word Embeddings", "abstract": "Contextualized word embeddings in language models have given much advance to NLP. Intuitively, sentential information is integrated into the representation of words, which can help model polysemy. However, context sensitivity also leads to the variance of representations, which may break the semantic consistency for synonyms. We quantify how much the contextualized embeddings of each word sense vary across contexts in typical pre-trained models. Results show that contextualized embeddings can be highly consistent across contexts. In addition, part-of-speech, number of word senses, and sentence length have an influence on the variance of sense representations. Interestingly, we find that word representations are position-biased, where the first words in different contexts tend to be more similar. We analyze such a phenomenon and also propose a simple way to alleviate such bias in distance-based word sense disambiguation settings.", "venue": "arXiv.org", "year": 2022, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2108102904", "name": "Yile Wang"}, {"authorId": "39939186", "name": "Yue Zhang"}]}}, {"contexts": [], "citingPaper": {"paperId": "15bacb240e2598457af4ded3039b6988aa9706f0", "externalIds": {"ACL": "2023.acl-long.102", "ArXiv": "2208.01009", "DBLP": "journals/corr/abs-2208-01009", "DOI": "10.48550/arXiv.2208.01009", "CorpusId": 251223486}, "url": "https://www.semanticscholar.org/paper/15bacb240e2598457af4ded3039b6988aa9706f0", "title": "Few-shot Adaptation Works with UnpredicTable Data", "abstract": "Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises FSL performance by a mean of +7.5% on 52 downstream tasks, which beats training on 40 human-curated NLP datasets (+6.7%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to FSL in general. We do not observe clear patterns between the datasets that lead to FSL gains, leaving open questions about why certain data helps with FSL.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 3, "influentialCitationCount": 1, "authors": [{"authorId": "2163950075", "name": "Jun Shern Chan"}, {"authorId": "15043672", "name": "M. Pieler"}, {"authorId": "2179879156", "name": "Jonathan Jao"}, {"authorId": "2070850274", "name": "J'er'emy Scheurer"}, {"authorId": "3439053", "name": "Ethan Perez"}]}}, {"contexts": ["In other cases, best performers find235 alternative acceptable answers but that do not overlap at all with the groundtruth.236\n5.3 Phrase Retrieval: In ranking, context only helps BERT embeddings but not others237\nPR is challenging but also meaningful for evaluating phrase embeddings because the embedding238 of the query is compared against that of all phrase candidates (extracted by tokenizers from the239 document), which are more often syntactically-incorrect phrases, meaningless phrases or rare phrases.240 This out-of-distribution challenge appears much less frequently in PS or WiC [27], i.e. non-retrieval241 benchmarks.", "After this step, \u223c17.96M phrases remain.111\nStep 4: Find phrases of ambiguous words To increase the chance of collecting ambigupus mNPs,112 we only keep mNPs that have at least one word in the list of 2,345 unique multiple-sense words of113 WiC [6], arriving at \u223c6.5M mNPs, each appearing in \u2265 2 sentences and in \u2265 1 Wikipedia pages.114\nStep 5: Find phrases of distinct contexts We observe that a mNP is likely to be ambiguous115 when (a) its context sentences are semantically different; and (b) its context Wikipedia pages are of116 dissimilar categories (e.g. \u201cmassive figure\u201d in finance vs. history; Fig.", "On312 PR-page, there may be more than one correct target phrase; however, we only have labels for one.313\nWhile WiC and English WSD rely on dictionaries to obtain word senses and example sentences, our314 data collection approach is dictionary-less, leveraging both NLP models and humans for annotation315 and therefore is extendable in the future to other types of phrases in the future.", "WiC: the Word-in-Context453 Dataset for Evaluating Context-Sensitive Meaning Representations.", "WiC [27], no such benchmarks exist 21 for phrases.", "Understanding phrases in context is a key to learning new vocabularies [24, 18], disambiguation [27], 17 and many downstream tasks, including semantic search [17].", "240 This out-of-distribution challenge appears much less frequently in PS or WiC [27], i.", "Without the need for predefined 80 senses, WiC [27] poses disambiguation as a binary classification task where the goal is to predict 81 whether a target word in two different sentences has the same or different meanings.", "While contextualized word embeddings18 in BERT [16] have been a major success, the contextualized phrase embeddings in existing systems19 mostly capture the common meaning of a phrase, i.e. without strongly relying on its context [36].20 While there are word-sense disambiguation datasets, e.g. WiC [27], no such benchmarks exist21 for phrases."], "citingPaper": {"paperId": "307c256abf29a0d96802b9cc8f48458ea58ca1bb", "externalIds": {"ArXiv": "2207.09068", "DBLP": "conf/eacl/PhamYBN23", "ACL": "2023.eacl-main.1", "DOI": "10.48550/arXiv.2207.09068", "CorpusId": 250462558}, "url": "https://www.semanticscholar.org/paper/307c256abf29a0d96802b9cc8f48458ea58ca1bb", "title": "PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search", "abstract": "While contextualized word embeddings have been a de-facto standard, learning contextualized phrase embeddings is less explored and being hindered by the lack of a human-annotated benchmark that tests machine understanding of phrase semantics given a context sentence or paragraph (instead of phrases alone). To fill this gap, we propose PiC\u2014a dataset of \u223c28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings. Training on PiC improves ranking-models\u2019 accuracy and remarkably pushes span selection (SS) models (i.e., predicting the start and end index of the target phrase) near human accuracy, which is 95% Exact Match (EM) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the SS models learn to better capture the common meaning of a phrase regardless of its actual context. SotA models perform poorly in distinguishing two senses of the same phrase in two contexts (\u223c60% EM) and in estimating the similarity between two different phrases in the same context (\u223c70% EM).", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2042922511", "name": "Thang M. Pham"}, {"authorId": "2110654003", "name": "Seunghyun Yoon"}, {"authorId": "145262461", "name": "Trung Bui"}, {"authorId": "151414531", "name": "Anh M Nguyen"}]}}, {"contexts": ["This variant generally yields similar or slightly worse performance compared to replacing one rationale at a time, but on the WiC task we observed a performance improvement (70.8% versus 65.2% when only one rationale is replaced), which indicates that this task might require greater rationale diversity to support strong task performance.", "Specifically, test split: ANLI, e-SNLI, OpenBookQA, ARC; dev/validation split: MNLI, RTE, BoolQ, Hotpot-QA, WiC, SST-2, QQP.", "For example, on e-SNLI, WiC and SST-2, the performance achieved by written-rationales is significantly worse than standard few-shot prompting without rationales, consistent with the findings in (Ye & Durrett, 2022).", "[WiC] Approach a task.", "We repeat this for every exemplar and report the final task performance using the new prompts in Figure 2 (denoted as sampled-r-k if the k-th rationale is replaced).\ne-SNLI Accuracy\nno-rationale\nhuman-written\nsampled-r1\nsampled-r2\nsampled-r3\nsampled-r4\n60 70 80 90\nBoolQ Accuracy\nno-rationale\nhuman-written\nsampled-r1\nsampled-r2\nsampled-r3\nsampled-r4\n60 65 70 75\nWiC Accuracy\nno-rationale\nhuman-written\nsampled-r1\nsampled-r2\nsampled-r3\nsampled-r4\n50 55 60 65 70\nSST-2 Accuracy\nno-rationale\nhuman-written\nsampled-r1\nsampled-r2\nsampled-r3\nsampled-r4\n80 85 90 95\nFigure 2: The performance varies depending on which rationales are used in the prompts for few-shot in-context learning.", "Here, for WiC, we evaluated an alternative variant of the input-rationale ensemble: instead of replacing one rationale in each prompt, we replace every original rationales by a generated one in each prompt.", "\u2026tasks where explicit intermediate steps might not be necessary, such as question answering (BoolQ; Clark et al., 2019), word sense disambiguation (WiC; Pilehvar & Camacho-Collados, 2019), sentiment analysis (SST-2; Socher et al., 2013), and paraphrase identification (QQP; Iyer et al., 2017).", "Perhaps surprisingly, we also find that the proposed framework can be used to improve few-shot learning in common natural language processing tasks, even including tasks where explicit intermediate steps might not be necessary, such as question answering (BoolQ; Clark et al., 2019), word sense disambiguation (WiC; Pilehvar & Camacho-Collados, 2019), sentiment analysis (SST-2; Socher et al., 2013), and paraphrase identification (QQP; Iyer et al., 2017).", "For WiC, ARC-easy/challenge and GSM8K, rationale-augmented ensembling outperforms both standard and chain-of-thought\nprompting by a large margin.", "Table 17: Few-shot exemplars for WiC.", "Similarly, for WiC, introducing greater diversity in sampled rationales improves performance (67.6", "\u2026of task performance to rationale quality across a range of natural language tasks, including e-SNLI (Camburu et al., 2018), BoolQ (Clark et al., 2019), WiC (Pilehvar & Camacho-Collados, 2019), and SST-2 (Socher et al., 2013), finding that human-generated rationales can indeed be sub-optimal.", "Once again, human-written rationales in few-shot learning can sometimes degrade performance compared to standard prompting (e.g., on RTE, OpenBookQA, WiC, ARC-challenge), while rationale-augmented ensembling with sampling in the output space (\u201coutput-sampled\u201d) reliably improves performance over both baselines.", "\u2022 Word Sense Disambiguation: Here we use Word-in-Context (WiC; Pilehvar & CamachoCollados, 2019).", "Given that rationale-augmented prompting has been shown to exhibit variable performance (Wei et al., 2022; Ye & Durrett, 2022), we first investigate the sensitivity of task performance to rationale quality across a range of natural language tasks, including e-SNLI (Camburu et al., 2018), BoolQ (Clark et al., 2019), WiC (Pilehvar & Camacho-Collados, 2019), and SST-2 (Socher et al., 2013), finding that human-generated rationales can indeed be sub-optimal."], "citingPaper": {"paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b", "externalIds": {"DBLP": "journals/corr/abs-2207-00747", "ArXiv": "2207.00747", "DOI": "10.48550/arXiv.2207.00747", "CorpusId": 250264890}, "url": "https://www.semanticscholar.org/paper/b17cc18e4130505b939f7d527082eb6be2a7fd5b", "title": "Rationale-Augmented Ensembles in Language Models", "abstract": "Recent research has shown that rationales, or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input ->output) prompts are expanded to (input, rationale ->output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a unified framework of rationale-augmented ensembles, where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales.", "venue": "arXiv.org", "year": 2022, "citationCount": 58, "influentialCitationCount": 15, "authors": [{"authorId": "1524732527", "name": "Xuezhi Wang"}, {"authorId": "119640649", "name": "Jason Wei"}, {"authorId": "50319359", "name": "D. Schuurmans"}, {"authorId": "1998340269", "name": "Quoc Le"}, {"authorId": "143829044", "name": "E. Chi"}, {"authorId": "65855107", "name": "Denny Zhou"}]}}, {"contexts": ["Consider the Word in Context (WiC) benchmark (Pilehvar and Camacho-Collados, 2019) shown in Figure 2H, as a historical example.", "Finally, Figure 2H shows the Word in Context (WiC) benchmark (Pilehvar and Camacho-Collados, 2019), which is a semantic understanding benchmark."], "citingPaper": {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "externalIds": {"DBLP": "journals/tmlr/WeiTBRZBYBZMCHVLDF22", "ArXiv": "2206.07682", "DOI": "10.48550/arXiv.2206.07682", "CorpusId": 249674500}, "url": "https://www.semanticscholar.org/paper/dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "venue": "Trans. Mach. Learn. Res.", "year": 2022, "citationCount": 615, "influentialCitationCount": 74, "authors": [{"authorId": "119640649", "name": "Jason Wei"}, {"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "150272855", "name": "Rishi Bommasani"}, {"authorId": "2402716", "name": "Colin Raffel"}, {"authorId": "2368067", "name": "Barret Zoph"}, {"authorId": "148016269", "name": "Sebastian Borgeaud"}, {"authorId": "1755465", "name": "Dani Yogatama"}, {"authorId": "40377863", "name": "Maarten Bosma"}, {"authorId": "65855107", "name": "Denny Zhou"}, {"authorId": "1680617", "name": "Donald Metzler"}, {"authorId": "143829044", "name": "E. Chi"}, {"authorId": "2117567142", "name": "Tatsunori Hashimoto"}, {"authorId": "1689108", "name": "Oriol Vinyals"}, {"authorId": "2075292388", "name": "P. Liang"}, {"authorId": "48448318", "name": "J. Dean"}, {"authorId": "26958176", "name": "W. Fedus"}]}}, {"contexts": ["We conduct experiments on a mixture of 34 NLP datasets (refer to Appendix B.2 for more details) grouped into ten task clusters, including both language understanding tasks and generation tasks:\n\u2022 Natural Language Inference: ANLI (R1-R3), CB, MNLI, QNLI, RTE, SNLI, WNLI \u2022 Sentiment Classification: IMDB, SST-2, Sentiment140, Yelp \u2022 Paraphrase Detection: QQP, MRPC, Paws Wiki \u2022 Coreference Resolution: DPR, Winogrande, WSC \u2022 Commonsense Reasoning: HellaSwag, PiQA, COPA \u2022 Reading Comprehension: DROP, SQuADv1, SQuADv2, OBQA, BoolQ \u2022 Miscellaneous: CoLA, WiC, TREC \u2022 Closed-Book QA: ARC-easy, NQ \u2022 Struct to Text: CommonGen, E2ENLG \u2022 Summarization: AESLC, SamSum, XSum", "\u2022 Miscellaneous consists of some additional datasets: CoLA (Warstadt et al., 2019), WiC (Pilehvar and Camacho-Collados, 2019), TREC (Li and Roth, 2002; Hovy et al., 2001)."], "citingPaper": {"paperId": "a8fd9c1625011741f74401ff9bdc1c584e25c86d", "externalIds": {"ArXiv": "2206.06336", "DBLP": "journals/corr/abs-2206-06336", "DOI": "10.48550/arXiv.2206.06336", "CorpusId": 249626024}, "url": "https://www.semanticscholar.org/paper/a8fd9c1625011741f74401ff9bdc1c584e25c86d", "title": "Language Models are General-Purpose Interfaces", "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.", "venue": "arXiv.org", "year": 2022, "citationCount": 46, "influentialCitationCount": 4, "authors": [{"authorId": "34128716", "name": "Y. Hao"}, {"authorId": "2187079427", "name": "Haoyu Song"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "3110003", "name": "Shaohan Huang"}, {"authorId": "46221722", "name": "Zewen Chi"}, {"authorId": "51456429", "name": "Wenhui Wang"}, {"authorId": "2118866998", "name": "Shuming Ma"}, {"authorId": "49807919", "name": "Furu Wei"}]}}, {"contexts": ["\u2026Zhang et al. 2018 superglue-rte cls/nli Dagan et al. 2005; Bar-Haim et al. 2006Giampiccolo et al. 2007; Bentivogli et al. 2009 superglue-wic cls/other Pilehvar and Camacho-Collados 2019 superglue-wsc cls/other Levesque et al. 2012 swag qa/multiple-choice qa Zellers et al. 2018 tab_fact cls/fact\u2026"], "citingPaper": {"paperId": "1da214f8f265445b5997f5d677452819b334bdfb", "externalIds": {"ArXiv": "2205.12701", "DBLP": "conf/emnlp/YeZR22", "DOI": "10.18653/v1/2022.findings-emnlp.189", "CorpusId": 253761592}, "url": "https://www.semanticscholar.org/paper/1da214f8f265445b5997f5d677452819b334bdfb", "title": "Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts", "abstract": "Recent works suggest that transformer models are capable of multi-tasking on diverse NLP tasks and adapting to new tasks efficiently. However, the potential of these multi-task models may be limited as they use the same set of parameters for all tasks. In contrast, humans tackle tasks in a more flexible way, by making proper presumptions on what skills and knowledge are relevant and executing only the necessary computations. Inspired by this, we propose to use task-level mixture-of-expert models, which has a collection of transformer layers (i.e., experts) and a router component that chooses from these experts dynamically and flexibly. We find that these models help improve the average performance gain (ARG) metric by 2.6% when adapting to unseen tasks in the few-shot setting and by 5.6% in the zero-shot generalization setting. Further, we show that the learned routing decisions partly rediscover human categorization of NLP tasks -- certain experts are strongly associated with extractive tasks, some with classification tasks, and some with tasks requiring world knowledge.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 1, "influentialCitationCount": 0, "authors": [{"authorId": "2075312420", "name": "Qinyuan Ye"}, {"authorId": "2329240", "name": "Juan Zha"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}}, {"contexts": ["We use 8 GLUE tasks (Wang et al., 2019b) and 5 SuperGLUE (Wang et al., 2019a) tasks as target datasets to test the model\u2019s natural language understanding abilities: BoolQ (Clark et al., 2019), CB (De Marneffe et al., 2019), MultiRC (Khashabi et al., 2018), WiC (Pilehvar and Camacho-Collados, 2019), WSC (Levesque et al., 2012), RTE (Giampiccolo et al., 2007), CoLA (Warstadt et al., 2019), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), MNLI, QQP, QNLI and SST-2.", ", 2018), WiC (Pilehvar and Camacho-Collados, 2019), WSC (Levesque et al.", "%) or WiC (48.9%)."], "citingPaper": {"paperId": "55a250868627de2d202d06e7cb3f6cbcd3a66f88", "externalIds": {"ArXiv": "2205.11961", "ACL": "2022.emnlp-main.446", "DBLP": "conf/emnlp/AsaiSPH22", "DOI": "10.18653/v1/2022.emnlp-main.446", "CorpusId": 254125751}, "url": "https://www.semanticscholar.org/paper/55a250868627de2d202d06e7cb3f6cbcd3a66f88", "title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts", "abstract": "This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts\u2014small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning), while it overcomes instability of prompt tuning and achieves high task performance using learned knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use 10 times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 21, "influentialCitationCount": 4, "authors": [{"authorId": "35584853", "name": "Akari Asai"}, {"authorId": "13082059", "name": "Mohammadreza Salehi"}, {"authorId": "39139825", "name": "Matthew E. Peters"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}]}}, {"contexts": [], "citingPaper": {"paperId": "4e5f7cd537a1bbcd090f9887b1b59f39a3715dba", "externalIds": {"ACL": "2023.acl-long.108", "DBLP": "journals/corr/abs-2205-10782", "ArXiv": "2205.10782", "DOI": "10.48550/arXiv.2205.10782", "CorpusId": 248986755}, "url": "https://www.semanticscholar.org/paper/4e5f7cd537a1bbcd090f9887b1b59f39a3715dba", "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions", "abstract": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 32, "influentialCitationCount": 9, "authors": [{"authorId": "1754700648", "name": "Or Honovich"}, {"authorId": "50482645", "name": "Uri Shaham"}, {"authorId": "3644767", "name": "Samuel R. Bowman"}, {"authorId": "39455775", "name": "Omer Levy"}]}}, {"contexts": ["natural language inference (ANLI [39], CB [40], and RTE [41]), coreference resolution (WSC [42] and Winogrande [43]), and word sense disambiguation (WiC [44])."], "citingPaper": {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "externalIds": {"DBLP": "journals/corr/abs-2205-05638", "ArXiv": "2205.05638", "DOI": "10.48550/arXiv.2205.05638", "CorpusId": 248693283}, "url": "https://www.semanticscholar.org/paper/7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning", "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 144, "influentialCitationCount": 34, "authors": [{"authorId": "48447436", "name": "Haokun Liu"}, {"authorId": "1390031652", "name": "Derek Tam"}, {"authorId": "1582888954", "name": "Mohammed Muqeeth"}, {"authorId": "117354463", "name": "Jay Mohta"}, {"authorId": "2110510944", "name": "Tenghao Huang"}, {"authorId": "143977268", "name": "Mohit Bansal"}, {"authorId": "2402716", "name": "Colin Raffel"}]}}, {"contexts": ["This is because a common way to represent OOV words is to take the average of their subwords (Pilehvar and CamachoCollados, 2019; Blevins and Zettlemoyer, 2020; Bommasani et al., 2020)."], "citingPaper": {"paperId": "dc80420810ab5828a53fcbf60fd385b7d2f672e9", "externalIds": {"ACL": "2022.findings-acl.164", "DBLP": "journals/corr/abs-2205-05093", "ArXiv": "2205.05093", "DOI": "10.48550/arXiv.2205.05093", "CorpusId": 248693403}, "url": "https://www.semanticscholar.org/paper/dc80420810ab5828a53fcbf60fd385b7d2f672e9", "title": "Richer Countries and Richer Representations", "abstract": "We examine whether some countries are more richly represented in embedding space than others. We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, \u201cThe country producing the most cocoa is [MASK].\u201d. Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country\u2019s GDP; thus perpetuating historic power and wealth inequalities. We analyze the effectiveness of mitigation strategies; recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.", "venue": "Findings", "year": 2022, "citationCount": 3, "influentialCitationCount": 0, "authors": [{"authorId": "3396547", "name": "Kaitlyn Zhou"}, {"authorId": "10324691", "name": "Kawin Ethayarajh"}, {"authorId": "1746807", "name": "Dan Jurafsky"}]}}, {"contexts": ["Using datasets with human similarity scores allows us to account for human perceived similarities when measuring the impact of frequency on cosine (Pilehvar and CamachoCollados, 2019; Huang et al., 2012)."], "citingPaper": {"paperId": "e05ace4ac80fd7af36ca183a8be5ef0b793d1482", "externalIds": {"DBLP": "conf/acl/ZhouECJ22", "ArXiv": "2205.05092", "ACL": "2022.acl-short.45", "DOI": "10.48550/arXiv.2205.05092", "CorpusId": 248693662}, "url": "https://www.semanticscholar.org/paper/e05ace4ac80fd7af36ca183a8be5ef0b793d1482", "title": "Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words", "abstract": "Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 10, "influentialCitationCount": 2, "authors": [{"authorId": "3396547", "name": "Kaitlyn Zhou"}, {"authorId": "10324691", "name": "Kawin Ethayarajh"}, {"authorId": "35540755", "name": "Dallas Card"}, {"authorId": "1746807", "name": "Dan Jurafsky"}]}}, {"contexts": [], "citingPaper": {"paperId": "81986b8a3d3fe6c5be06fc4527953fb514ad12e8", "externalIds": {"DBLP": "conf/naacl/ChenDPMISK22", "ArXiv": "2205.01703", "ACL": "2022.naacl-main.260", "DOI": "10.48550/arXiv.2205.01703", "CorpusId": 248512524}, "url": "https://www.semanticscholar.org/paper/81986b8a3d3fe6c5be06fc4527953fb514ad12e8", "title": "Improving In-Context Few-Shot Learning via Self-Supervised Training", "abstract": "Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2022, "citationCount": 16, "influentialCitationCount": 3, "authors": [{"authorId": "46221498", "name": "Mingda Chen"}, {"authorId": "3048577", "name": "Jingfei Du"}, {"authorId": "10721120", "name": "Ramakanth Pasunuru"}, {"authorId": "39980906", "name": "Todor Mihaylov"}, {"authorId": "1900163", "name": "Srini Iyer"}, {"authorId": "1389924486", "name": "Ves Stoyanov"}, {"authorId": "1714932", "name": "Zornitsa Kozareva"}]}}, {"contexts": [], "citingPaper": {"paperId": "07c70ca55793984ffdf31582a05170ef3d62381a", "externalIds": {"DBLP": "journals/corr/abs-2205-00049", "ArXiv": "2205.00049", "DOI": "10.48550/arXiv.2205.00049", "CorpusId": 248496641}, "url": "https://www.semanticscholar.org/paper/07c70ca55793984ffdf31582a05170ef3d62381a", "title": "Prompt Consistency for Zero-Shot Task Generalization", "abstract": "One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 29, "influentialCitationCount": 4, "authors": [{"authorId": "2110714400", "name": "Chunting Zhou"}, {"authorId": "6215698", "name": "Junxian He"}, {"authorId": "2378954", "name": "Xuezhe Ma"}, {"authorId": "1400419309", "name": "Taylor Berg-Kirkpatrick"}, {"authorId": "2075395906", "name": "Graham Neubig"}]}}, {"contexts": [], "citingPaper": {"paperId": "51b950bfcaba4bad321e7342b32833d42f42c914", "externalIds": {"ArXiv": "2204.07689", "DBLP": "journals/corr/abs-2204-07689", "DOI": "10.48550/arXiv.2204.07689", "CorpusId": 248227728}, "url": "https://www.semanticscholar.org/paper/51b950bfcaba4bad321e7342b32833d42f42c914", "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners", "abstract": "Traditional multi-task learning (MTL) methods use dense networks that use the same set of shared weights across several different tasks. This often creates interference where two or more tasks compete to pull model parameters in different directions. In this work, we study whether sparsely activated Mixture-of-Experts (MoE) improve multi-task learning by specializing some weights for learning shared representations and using the others for learning task-specific information. To this end, we devise task-aware gating functions to route examples from different tasks to specialized experts which share subsets of network weights conditioned on the task. This results in a sparsely activated multi-task model with a large number of parameters, but with the same computational cost as that of a dense model. We demonstrate such sparse networks to improve multi-task learning along three key dimensions: (i) transfer to low-resource tasks from related tasks in the training mixture; (ii) sample-efficient generalization to tasks not seen during training by making use of task-aware routing from seen related tasks; (iii) robustness to the addition of unrelated tasks by avoiding catastrophic forgetting of existing tasks.", "venue": "arXiv.org", "year": 2022, "citationCount": 12, "influentialCitationCount": 1, "authors": [{"authorId": "2152953535", "name": "Shashank Gupta"}, {"authorId": "2153292652", "name": "Subhabrata Mukherjee"}, {"authorId": "2043231778", "name": "K. Subudhi"}, {"authorId": "2162804727", "name": "Eduardo Gonzalez"}, {"authorId": "144430856", "name": "Damien Jose"}, {"authorId": "2072795428", "name": "A. Awadallah"}, {"authorId": "48441311", "name": "Jianfeng Gao"}]}}, {"contexts": [], "citingPaper": {"paperId": "c87efea624fcc9aac240b80d548ca9225a74e053", "externalIds": {"ArXiv": "2204.04541", "DBLP": "journals/corr/abs-2204-04541", "ACL": "2022.coling-1.325", "DOI": "10.48550/arXiv.2204.04541", "CorpusId": 248085548}, "url": "https://www.semanticscholar.org/paper/c87efea624fcc9aac240b80d548ca9225a74e053", "title": "KoBEST: Korean Balanced Evaluation of Significant Tasks", "abstract": "A well-formulated benchmark plays a critical role in spurring advancements in the natural language processing (NLP) field, as it allows objective and precise evaluation of diverse models. As modern language models (LMs) have become more elaborate and sophisticated, more difficult benchmarks that require linguistic knowledge and reasoning have been proposed. However, most of these benchmarks only support English, and great effort is necessary to construct benchmarks for other low resource languages. To this end, we propose a new benchmark named Korean balanced evaluation of significant tasks (KoBEST), which consists of five Korean-language downstream tasks. Professional Korean linguists designed the tasks that require advanced Korean linguistic knowledge. Moreover, our data is purely annotated by humans and thoroughly reviewed to guarantee high data quality. We also provide baseline models and human performance results. Our dataset is available on the Huggingface.", "venue": "International Conference on Computational Linguistics", "year": 2022, "citationCount": 3, "influentialCitationCount": 0, "authors": [{"authorId": "2111405314", "name": "Dohyeong Kim"}, {"authorId": "35756238", "name": "Myeongjun Jang"}, {"authorId": "79300918", "name": "D. Kwon"}, {"authorId": "2052286094", "name": "Eric Davis"}]}}, {"contexts": [], "citingPaper": {"paperId": "aa8f3e081ad2869c9469e2726364bdae0d9bdc7f", "externalIds": {"DBLP": "journals/corr/abs-2204-03044", "ArXiv": "2204.03044", "DOI": "10.48550/arXiv.2204.03044", "CorpusId": 248006458}, "url": "https://www.semanticscholar.org/paper/aa8f3e081ad2869c9469e2726364bdae0d9bdc7f", "title": "Fusing finetuned models for better pretraining", "abstract": "Pretrained models are the standard starting point for training. This approach consistently outperforms the use of a random initialization. However, pretraining is a costly endeavour that few can undertake. In this paper, we create better base models at hardly any cost, by fusing multiple existing fine tuned models into one. Specifically, we fuse by averaging the weights of these models. We show that the fused model results surpass the pretrained model ones. We also show that fusing is often better than intertraining. We find that fusing is less dependent on the target task. Furthermore, weight decay nullifies intertraining effects but not those of fusing.", "venue": "arXiv.org", "year": 2022, "citationCount": 23, "influentialCitationCount": 3, "authors": [{"authorId": "41019330", "name": "Leshem Choshen"}, {"authorId": "5598623", "name": "Elad Venezian"}, {"authorId": "1766595", "name": "N. Slonim"}, {"authorId": "1722434", "name": "Yoav Katz"}]}}], "next": 100}, "references": {"offset": 0, "data": [{"contexts": ["Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words(1), in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018)."], "citedPaper": {"paperId": "bf9db8ca2dce7386cbed1ae0fd6465148cdb2b98", "externalIds": {"MAG": "2799894091", "ArXiv": "1805.04032", "DBLP": "journals/corr/abs-1805-04032", "DOI": "10.1613/JAIR.1.11259", "CorpusId": 13696533}, "url": "https://www.semanticscholar.org/paper/bf9db8ca2dce7386cbed1ae0fd6465148cdb2b98", "title": "From Word to Sense Embeddings: A Survey on Vector Representations of Meaning", "abstract": "\n \n \nOver the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality. \n \n \n", "venue": "Journal of Artificial Intelligence Research", "year": 2018, "citationCount": 286, "influentialCitationCount": 11, "authors": [{"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}, {"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}]}}, {"contexts": ["contextualized word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word.", "ELMo (Peters et al., 2018) is a character-based model which learns dynamic word embeddings that can change depending on the context.", "\u2026words are important as they constitute the most frequent words in a natural language (Zipf, 1949).\ncontextualized word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word.", "ELMo (Peters et al., 2018) is a character-based model which learns dynamic"], "citedPaper": {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "externalIds": {"MAG": "2962739339", "DBLP": "journals/corr/abs-1802-05365", "ArXiv": "1802.05365", "ACL": "N18-1202", "DOI": "10.18653/v1/N18-1202", "CorpusId": 3626819}, "url": "https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2018, "citationCount": 9858, "influentialCitationCount": 1589, "authors": [{"authorId": "39139825", "name": "Matthew E. Peters"}, {"authorId": "50043859", "name": "Mark Neumann"}, {"authorId": "2136562", "name": "Mohit Iyyer"}, {"authorId": "40642935", "name": "Matt Gardner"}, {"authorId": "143997772", "name": "Christopher Clark"}, {"authorId": "2544107", "name": "Kenton Lee"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}}, {"contexts": ["This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (Ru\u0308d et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017)."], "citedPaper": {"paperId": "5cc2204e1f74101d3076ebddefeed17b48596f74", "externalIds": {"DBLP": "journals/corr/abs-1710-06632", "MAG": "2740714844", "ArXiv": "1710.06632", "ACL": "P17-1170", "DOI": "10.18653/v1/P17-1170", "CorpusId": 22604822}, "url": "https://www.semanticscholar.org/paper/5cc2204e1f74101d3076ebddefeed17b48596f74", "title": "Towards a Seamless Integration of Word Senses into Downstream NLP Applications", "abstract": "Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2017, "citationCount": 45, "influentialCitationCount": 0, "authors": [{"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}, {"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}, {"authorId": "1733928", "name": "Roberto Navigli"}, {"authorId": "3072167", "name": "Nigel Collier"}]}}, {"contexts": ["\u2026(2) it is framed as a binary classification dataset, in which, unlike SCWS, identical words\n2With a similar goal in mind but focused on hypernymy, Vyas and Carpuat (2017) developed a benchmark to assess the capability of automatic systems to detect hypernymy relations in context.\nare paired with\u2026"], "citedPaper": {"paperId": "675b5c84b68ed00c118e919a93c36c05cece14dc", "externalIds": {"MAG": "2741398834", "DBLP": "conf/starsem/VyasC17", "ACL": "S17-1004", "DOI": "10.18653/v1/S17-1004", "CorpusId": 30176802}, "url": "https://www.semanticscholar.org/paper/675b5c84b68ed00c118e919a93c36c05cece14dc", "title": "Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection", "abstract": "We introduce WHiC, a challenging testbed for detecting hypernymy, an asymmetric relation between words. While previous work has focused on detecting hypernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts. WHiC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context. We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.", "venue": "Joint Conference on Lexical and Computational Semantics", "year": 2017, "citationCount": 8, "influentialCitationCount": 0, "authors": [{"authorId": "2065005", "name": "Yogarshi Vyas"}, {"authorId": "2954727", "name": "Marine Carpuat"}]}}, {"contexts": ["SW2V13 (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, producing a shared vector space of words and senses as a result.", "\u2026and there are different strategies to perform this sense\n4Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset.\ndistinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017)."], "citedPaper": {"paperId": "0019c072a19c6296e3ed90056fb8caa79b10ab02", "externalIds": {"ACL": "K17-1012", "DBLP": "journals/corr/ManciniCIN16", "MAG": "2949267551", "ArXiv": "1612.02703", "DOI": "10.18653/v1/K17-1012", "CorpusId": 3003897}, "url": "https://www.semanticscholar.org/paper/0019c072a19c6296e3ed90056fb8caa79b10ab02", "title": "Embedding Words and Senses Together via Joint Knowledge-Enhanced Training", "abstract": "Word embeddings are widely used in Natural Language Processing, mainly due to their success in capturing semantic information from massive corpora. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new model which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.", "venue": "Conference on Computational Natural Language Learning", "year": 2016, "citationCount": 80, "influentialCitationCount": 14, "authors": [{"authorId": "38286801", "name": "Massimiliano Mancini"}, {"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}, {"authorId": "2676143", "name": "Ignacio Iacobacci"}, {"authorId": "1733928", "name": "Roberto Navigli"}]}}, {"contexts": ["DeConf(12) (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet."], "citedPaper": {"paperId": "b54315a22b825e9ca1b59aa1d3fac98ea4925941", "externalIds": {"ACL": "D16-1174", "ArXiv": "1608.01961", "DBLP": "conf/emnlp/PilehvarC16", "MAG": "2963706278", "DOI": "10.18653/v1/D16-1174", "CorpusId": 16173223}, "url": "https://www.semanticscholar.org/paper/b54315a22b825e9ca1b59aa1d3fac98ea4925941", "title": "De-Conflated Semantic Representations", "abstract": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2016, "citationCount": 88, "influentialCitationCount": 13, "authors": [{"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}, {"authorId": "3072167", "name": "Nigel Collier"}]}}, {"contexts": ["JBT(11) (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embeddings and computes embedding for each cluster (sense).", "For these three methods we follow the disambiguation strategy suggested by Pelevina et al. (2016): for each example we retrieve the closest sense embedding to the context vector, which is computed by averaging its contained words\u2019 embeddings.", "in two categories: multi-prototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and", "\u2026of proposals have been put forward, mainly in two categories: multi-prototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words,\u2026", "JBT11 (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embeddings and computes embedding for each cluster (sense)."], "citedPaper": {"paperId": "b4e3e8f1b8ca595b6ab3b28aaf39fcc5f338dac3", "externalIds": {"DBLP": "journals/corr/abs-1708-03390", "MAG": "2952827547", "ACL": "W16-1620", "ArXiv": "1708.03390", "DOI": "10.18653/v1/W16-1620", "CorpusId": 5999791}, "url": "https://www.semanticscholar.org/paper/b4e3e8f1b8ca595b6ab3b28aaf39fcc5f338dac3", "title": "Making Sense of Word Embeddings", "abstract": "We present a simple yet effective approach for learning word sense embeddings. In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems.", "venue": "Rep4NLP@ACL", "year": 2016, "citationCount": 128, "influentialCitationCount": 15, "authors": [{"authorId": "39907092", "name": "Maria Pelevina"}, {"authorId": "152397221", "name": "Nikolay Arefiev"}, {"authorId": "31565315", "name": "Chris Biemann"}, {"authorId": "144033941", "name": "Alexander Panchenko"}]}}, {"contexts": ["contextualized word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word.", "the pioneering contextualized word embedding models is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context using a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) language model.", "\u2026and\n1Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949).\ncontextualized word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary\u2026", "One of the pioneering contextualized word embedding models is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context using a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) language model."], "citedPaper": {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "externalIds": {"DBLP": "conf/conll/MelamudGD16", "ACL": "K16-1006", "MAG": "2507974895", "DOI": "10.18653/v1/K16-1006", "CorpusId": 7890036}, "url": "https://www.semanticscholar.org/paper/59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM", "abstract": "Context representations are central to various NLP tasks, such as word sense disam-biguation, named entity recognition, co-reference resolution, and many more. In this work we present a neural model for ef\ufb01ciently learning a generic context embedding function from large corpora, us-ing bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pre-trained models, suggesting they could be useful in a wide variety of NLP tasks.", "venue": "Conference on Computational Natural Language Learning", "year": 2016, "citationCount": 447, "influentialCitationCount": 51, "authors": [{"authorId": "2298649", "name": "Oren Melamud"}, {"authorId": "34508613", "name": "J. Goldberger"}, {"authorId": "7465342", "name": "Ido Dagan"}]}}, {"contexts": ["This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (Ru\u0308d et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017)."], "citedPaper": {"paperId": "9c7c96521eb20b09733378af30c3335466f7a175", "externalIds": {"DBLP": "conf/acl/FlekovaG16", "ACL": "P16-1191", "MAG": "2511478665", "DOI": "10.18653/v1/P16-1191", "CorpusId": 1490918}, "url": "https://www.semanticscholar.org/paper/9c7c96521eb20b09733378af30c3335466f7a175", "title": "Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization", "abstract": "Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation. To date, no effort has been put into integrating the supersenses into distributional word representations. We present a novel joint embedding model of words and supersenses, providing insights into the relationship between words and supersenses in the same vector space. Using these embeddings in a deep neural network model, we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2016, "citationCount": 58, "influentialCitationCount": 4, "authors": [{"authorId": "2192277", "name": "Lucie Flekova"}, {"authorId": "1730400", "name": "Iryna Gurevych"}]}}, {"contexts": ["Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense\n4Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding\u2026", "Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense"], "citedPaper": {"paperId": "d9cae679aea6e288fd8c7085074972c084776f0c", "externalIds": {"DBLP": "journals/coling/McCarthyAE16", "ACL": "J16-2003", "MAG": "2303034160", "DOI": "10.1162/COLI_a_00247", "CorpusId": 15493560}, "url": "https://www.semanticscholar.org/paper/d9cae679aea6e288fd8c7085074972c084776f0c", "title": "Word Sense Clustering and Clusterability", "abstract": "Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses. But this seems to be a much easier task for some lemmas than others. Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability. Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics.We propose to operationalize partitionability as clusterability, a measure of how easy the occurrences of a lemma are to cluster. We test two ways of measuring clusterability: (1) existing measures from the machine learning literature that aim to measure the goodness of optimal k-means clusterings, and (2) the idea that if a lemma is more clusterable, two clusterings based on two different \u201cviews\u201d of the same data points will be more congruent. The two views that we use are two different sets of manually constructed lexical substitutes for the target lemma, on the one hand monolingual paraphrases, and on the other hand translations. We apply automatic clustering to the manual annotations. We use manual annotations because we want the representations of the instances that we cluster to be as informative and \u201cclean\u201d as possible. We show that when we control for polysemy, our measures of clusterability tend to correlate with partitionability, in particular some of the type-(1) clusterability measures, and that these measures outperform a baseline that relies on the amount of overlap in a soft clustering.", "venue": "International Conference on Computational Logic", "year": 2016, "citationCount": 40, "influentialCitationCount": 4, "authors": [{"authorId": "145586618", "name": "Diana McCarthy"}, {"authorId": "2817917", "name": "Marianna Apidianaki"}, {"authorId": "1708114", "name": "K. Erk"}]}}, {"contexts": ["\u2026around this limitation dozens of proposals have been put forward, mainly in two categories: multi-prototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual\u2026"], "citedPaper": {"paperId": "1a08e135ac11db0249c6afb4540672c5a349495e", "externalIds": {"ACL": "D14-1113", "ArXiv": "1504.06654", "DBLP": "journals/corr/NeelakantanSPM15", "MAG": "2949364118", "DOI": "10.3115/v1/D14-1113", "CorpusId": 15251438}, "url": "https://www.semanticscholar.org/paper/1a08e135ac11db0249c6afb4540672c5a349495e", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "abstract": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type\u2014ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2014, "citationCount": 447, "influentialCitationCount": 69, "authors": [{"authorId": "2072676", "name": "Arvind Neelakantan"}, {"authorId": "2064977892", "name": "Jeevan Shankar"}, {"authorId": "144720379", "name": "Alexandre Passos"}, {"authorId": "143753639", "name": "A. McCallum"}]}}, {"contexts": [], "citedPaper": {"paperId": "7a96765c147c9c814803c8c9de28a1dd069271da", "externalIds": {"ArXiv": "1408.3456", "ACL": "J15-4004", "MAG": "1854884267", "DBLP": "journals/coling/HillRK15", "DOI": "10.1162/COLI_a_00237", "CorpusId": 3226120}, "url": "https://www.semanticscholar.org/paper/7a96765c147c9c814803c8c9de28a1dd069271da", "title": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation", "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.", "venue": "International Conference on Computational Logic", "year": 2014, "citationCount": 1197, "influentialCitationCount": 213, "authors": [{"authorId": "145783676", "name": "Felix Hill"}, {"authorId": "1762757", "name": "Roi Reichart"}, {"authorId": "145762466", "name": "A. Korhonen"}]}}, {"contexts": [], "citedPaper": {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "externalIds": {"DBLP": "conf/nips/MikolovSCCD13", "ArXiv": "1310.4546", "MAG": "2153579005", "CorpusId": 16447573}, "url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "venue": "NIPS", "year": 2013, "citationCount": 30072, "influentialCitationCount": 4196, "authors": [{"authorId": "2047446108", "name": "Tomas Mikolov"}, {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "2118440152", "name": "Kai Chen"}, {"authorId": "32131713", "name": "G. Corrado"}, {"authorId": "49959210", "name": "J. Dean"}]}}, {"contexts": [], "citedPaper": {"paperId": "f2bc2c9ca182def54be95f32ba82807c33d30b6b", "externalIds": {"DBLP": "conf/acl/PilehvarJN13", "ACL": "P13-1132", "MAG": "2251797829", "CorpusId": 17017087}, "url": "https://www.semanticscholar.org/paper/f2bc2c9ca182def54be95f32ba82807c33d30b6b", "title": "Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity", "abstract": "Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2013, "citationCount": 197, "influentialCitationCount": 23, "authors": [{"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}, {"authorId": "3046220", "name": "David Jurgens"}, {"authorId": "1733928", "name": "Roberto Navigli"}]}}, {"contexts": ["SW2V13 (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, producing a shared vector space of words and senses as a result.", "The system makes use of Word2vec (Mikolov et al., 2013b) 300-d embeddings pretrained on the Google News corpus.", "For instance, we computed the performance of the Google News Word2vec pretrained word embeddings (Mikolov et al., 2013b) on the dataset to be 0.65 (\u03c1), which is significantly higher than the optimistic IRA for the dataset."], "citedPaper": {"paperId": "330da625c15427c6e42ccfa3b747fb29e5835bf0", "externalIds": {"MAG": "2950577311", "DBLP": "journals/corr/abs-1301-3781", "ArXiv": "1301.3781", "CorpusId": 5959482}, "url": "https://www.semanticscholar.org/paper/330da625c15427c6e42ccfa3b747fb29e5835bf0", "title": "Efficient Estimation of Word Representations in Vector Space", "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.", "venue": "International Conference on Learning Representations", "year": 2013, "citationCount": 26705, "influentialCitationCount": 4150, "authors": [{"authorId": "2047446108", "name": "Tomas Mikolov"}, {"authorId": "2118440152", "name": "Kai Chen"}, {"authorId": "32131713", "name": "G. Corrado"}, {"authorId": "49959210", "name": "J. Dean"}]}}, {"contexts": ["We used WordNet as our core resource, exploiting BabelNet\u2019s mappings (Navigli and Ponzetto, 2012) as a bridge between Wiktionary and VerbNet to WordNet."], "citedPaper": {"paperId": "7f90ef42f22d4f9b86d33b0ad7f16261273c8612", "externalIds": {"DBLP": "journals/ai/NavigliP12", "MAG": "2120699290", "DOI": "10.1016/J.ARTINT.2012.07.001", "CorpusId": 6063065}, "url": "https://www.semanticscholar.org/paper/7f90ef42f22d4f9b86d33b0ad7f16261273c8612", "title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "abstract": null, "venue": "Artificial Intelligence", "year": 2012, "citationCount": 1552, "influentialCitationCount": 243, "authors": [{"authorId": "1733928", "name": "Roberto Navigli"}, {"authorId": "1801255", "name": "Simone Paolo Ponzetto"}]}}, {"contexts": ["(SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to stan-", "To our knowledge, the Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) is the only existing benchmark that specifically focuses on the dynamic nature of word semantics.2 In Section 4 we will explain the limitations of this dataset for the evaluation of recent work in the\u2026", "The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015), in which the task is to automatically estimate the semantic\u2026", "To our knowledge, the Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) is the only existing benchmark that specifically focuses on the dynamic nature of word semantics."], "citedPaper": {"paperId": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "externalIds": {"MAG": "2164019165", "ACL": "P12-1092", "DBLP": "conf/acl/HuangSMN12", "CorpusId": 372093}, "url": "https://www.semanticscholar.org/paper/2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2012, "citationCount": 1252, "influentialCitationCount": 158, "authors": [{"authorId": "40150953", "name": "E. Huang"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId": "144783904", "name": "Christopher D. Manning"}, {"authorId": "34699434", "name": "A. Ng"}]}}, {"contexts": ["This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (Ru\u0308d et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017)."], "citedPaper": {"paperId": "0c432aedd60e73dc19a707f4a1d967f089c7b4b7", "externalIds": {"ACL": "P11-1097", "DBLP": "conf/acl/RudCMS11", "MAG": "2102292252", "CorpusId": 11249236}, "url": "https://www.semanticscholar.org/paper/0c432aedd60e73dc19a707f4a1d967f089c7b4b7", "title": "Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition", "abstract": "We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2011, "citationCount": 39, "influentialCitationCount": 2, "authors": [{"authorId": "2387016", "name": "Stefan R\u00fcd"}, {"authorId": "2754495", "name": "Massimiliano Ciaramita"}, {"authorId": "2110852656", "name": "J. M\u00fcller"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}}, {"contexts": ["\u2026Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015), in which the task is to automatically estimate the semantic similarity of word pairs.", "The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015), in which the task is to automatically estimate the semantic similarity of word pairs."], "citedPaper": {"paperId": "c62450a13bb6692385490dd4b371de9857761374", "externalIds": {"MAG": "2164973920", "DBLP": "conf/naacl/ReisingerM10", "ACL": "N10-1013", "CorpusId": 2156506}, "url": "https://www.semanticscholar.org/paper/c62450a13bb6692385490dd4b371de9857761374", "title": "Multi-Prototype Vector-Space Models of Word Meaning", "abstract": "Current vector-space models of lexical semantics create a single \"prototype\" vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple \"sense-specific\" vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2010, "citationCount": 416, "influentialCitationCount": 43, "authors": [{"authorId": "2432655", "name": "J. Reisinger"}, {"authorId": "1797655", "name": "R. Mooney"}]}}, {"contexts": ["\u2026and there are different strategies to perform this sense\n4Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset.\ndistinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017).", ", 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017)."], "citedPaper": {"paperId": "a88966cdaeddd15d0a3de365a8f0a5931aebd756", "externalIds": {"MAG": "234105", "ACL": "D07-1107", "DBLP": "conf/emnlp/SnowPJN07", "CorpusId": 6481231}, "url": "https://www.semanticscholar.org/paper/a88966cdaeddd15d0a3de365a8f0a5931aebd756", "title": "Learning to Merge Word Senses", "abstract": "It has been widely observed that different NLP applications require different sense granularities in order to best exploit word sense distinctions, and that for many applications WordNet senses are too fine-grained. In contrast to previously proposed automatic methods for sense clustering, we formulate sense merging as a supervised learning problem, exploiting human-labeled sense clusterings as training data. We train a discriminative classifier over a wide variety of features derived from WordNet structure, corpus-based evidence, and evidence from other lexical resources. Our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments, yielding an absolute F-score improvement of 4.1% on nouns, 13.6% on verbs, and 4.0% on adjectives. Finally, we propose a model for clustering sense taxonomies using the outputs of our classifier, and we make available several automatically sense-clustered WordNets of various sense granularities.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2007, "citationCount": 119, "influentialCitationCount": 13, "authors": [{"authorId": "144621026", "name": "R. Snow"}, {"authorId": "28612243", "name": "Sushant Prakash"}, {"authorId": "1746807", "name": "Dan Jurafsky"}, {"authorId": "34699434", "name": "A. Ng"}]}}, {"contexts": ["WordNet is known to be a fine-grained resource (Navigli, 2006)."], "citedPaper": {"paperId": "f599522c03471ccec0cdc0bc98b3f8dd7ed4ecf1", "externalIds": {"MAG": "2135824681", "DBLP": "conf/acl/Navigli06", "ACL": "P06-1014", "DOI": "10.3115/1220175.1220189", "CorpusId": 6254130}, "url": "https://www.semanticscholar.org/paper/f599522c03471ccec0cdc0bc98b3f8dd7ed4ecf1", "title": "Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance", "abstract": "Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2006, "citationCount": 185, "influentialCitationCount": 21, "authors": [{"authorId": "1733928", "name": "Roberto Navigli"}]}}, {"contexts": ["We used two simple binary classifiers in our experiments on top of all comparison systems (except for the LSTM baseline).", "We\n7https://github.com/orenmel/context2vec\nused the 1024-d pre-trained models8 for two configurations: ELMo1, the first LSTM hidden state, and ELMo3, the weighted sum of the 3 layers of LSTM.", ", 2016), which computes the embedding for a word in context using a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) language model.", "Sentence LSTM is another baseline, which differently from the other models, does not obtain explicit encoded representations of the target word or sentence.", "The system has two LSTM layers with 50 units, one for each context side, which concatenates the outputs and passes that to a feedforward layer with 64 neurons, followed by a dropout layer at rate 0.5, and a final one-neuron output layer of sigmoid activation.", "One of the pioneering contextualized word embedding models is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context using a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) language model.", "However, surprisingly, neither ELMo nor Context2vec are able to improve over the simple sentence BoW baseline (which also outperforms the sentence LSTM baseline) using the threshold strategy.", "ELMo embeddings are essentially the internal states of a deep LSTM-based language model, pre-trained on a large text corpus."], "citedPaper": {"paperId": "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "externalIds": {"DBLP": "journals/neco/HochreiterS97", "MAG": "2064675550", "DOI": "10.1162/neco.1997.9.8.1735", "CorpusId": 1915014, "PubMed": "9377276"}, "url": "https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9", "title": "Long Short-Term Memory", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "venue": "Neural Computation", "year": 1997, "citationCount": 67254, "influentialCitationCount": 11789, "authors": [{"authorId": "3308557", "name": "S. Hochreiter"}, {"authorId": "145341374", "name": "J. Schmidhuber"}]}}, {"contexts": ["dard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al.", "The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015), in which the task is to automatically estimate the semantic similarity of word pairs."], "citedPaper": {"paperId": "7ef3ac14cdb484aaa2b039850093febd5cf73a21", "externalIds": {"DBLP": "journals/cacm/RubensteinG65", "MAG": "2080100102", "DOI": "10.1145/365628.365657", "CorpusId": 18309234}, "url": "https://www.semanticscholar.org/paper/7ef3ac14cdb484aaa2b039850093febd5cf73a21", "title": "Contextual correlates of synonymy", "abstract": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.", "venue": "CACM", "year": 1965, "citationCount": 1538, "influentialCitationCount": 180, "authors": [{"authorId": "47709773", "name": "H. Rubenstein"}, {"authorId": "1898344", "name": "J. Goodenough"}]}}, {"contexts": [], "citedPaper": {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "externalIds": {"ArXiv": "1810.04805", "DBLP": "conf/naacl/DevlinCLT19", "ACL": "N19-1423", "MAG": "2951055169", "DOI": "10.18653/v1/N19-1423", "CorpusId": 52967399}, "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2019, "citationCount": 58367, "influentialCitationCount": 17538, "authors": [{"authorId": "39172707", "name": "Jacob Devlin"}, {"authorId": "1744179", "name": "Ming-Wei Chang"}, {"authorId": "2544107", "name": "Kenton Lee"}, {"authorId": "3259253", "name": "Kristina Toutanova"}]}}, {"contexts": ["In fact, Dubossarsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets.", "In fact, Dubossarsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling"], "citedPaper": {"paperId": "21b0bb7cd24a492ab5dacb9be14bce8993f99bb3", "externalIds": {"ACL": "D18-1200", "MAG": "2889647547", "DBLP": "conf/emnlp/DubossarskyGW18", "DOI": "10.18653/v1/D18-1200", "CorpusId": 52216759}, "url": "https://www.semanticscholar.org/paper/21b0bb7cd24a492ab5dacb9be14bce8993f99bb3", "title": "Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research", "abstract": "The point of departure of this article is the claim that sense-specific vectors provide an advantage over normal vectors due to the polysemy that they presumably represent. This claim is based on performance gains observed in gold standard evaluation tests such as word similarity tasks. We demonstrate that this claim, at least as it is instantiated in prior art, is unfounded in two ways. Furthermore, we provide empirical data and an analytic discussion that may account for the previously reported improved performance. First, we show that ground-truth polysemy degrades performance in word similarity tasks. Therefore word similarity tasks are not suitable as an evaluation test for polysemy representation. Second, random assignment of words to senses is shown to improve performance in the same task. This and additional results point to the conclusion that performance gains as reported in previous work may be an artifact of random sense assignment, which is equivalent to sub-sampling and multiple estimation of word vector representations. Theoretical analysis shows that this may on its own be beneficial for the estimation of word similarity, by reducing the bias in the estimation of the cosine distance.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2018, "citationCount": 15, "influentialCitationCount": 1, "authors": [{"authorId": "2026652", "name": "Haim Dubossarsky"}, {"authorId": "152679789", "name": "Eitan Grossman"}, {"authorId": "1789171", "name": "D. Weinshall"}]}}, {"contexts": ["This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R\u00fcd et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017).", "This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (Ru\u0308d et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017)."], "citedPaper": {"paperId": "79e5966a2355fb40f306e380dba58a155db4d609", "externalIds": {"MAG": "2250387780", "ACL": "P13-2125", "DBLP": "conf/acl/SeverynNM13", "CorpusId": 6431532}, "url": "https://www.semanticscholar.org/paper/79e5966a2355fb40f306e380dba58a155db4d609", "title": "Learning Semantic Textual Similarity with Structural Representations", "abstract": "Measuring semantic textual similarity (STS) is at the cornerstone of many NLP applications. Different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following: (i) it directly encodes input texts into relational syntactic structures; (ii) relies on tree kernels to handle feature engineering automatically; (iii) combines both structural and feature vector representations in a single scoring model, i.e., in Support Vector Regression (SVR); and (iv) delivers significant improvement over the best STS systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2013, "citationCount": 41, "influentialCitationCount": 2, "authors": [{"authorId": "3091861", "name": "Aliaksei Severyn"}, {"authorId": "1735421", "name": "M. Nicosia"}, {"authorId": "1719404", "name": "Alessandro Moschitti"}]}}, {"contexts": ["Contextual sentences in WiC were extracted from example usages provided for words in three lexical resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) VerbNet (Kipper-Schuler, 2005), the largest domainindependent verb-based resource; and (3) Wiktionary3, a large collaborative-constructed online dictionary.", "\u2026example usages provided for words in three lexical resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) VerbNet (Kipper-Schuler, 2005), the largest domainindependent verb-based resource; and (3) Wiktionary3, a large collaborative-constructed online dictionary.", "4 The total number of initial examples extracted from all resources at this stage were 23,949, 10,564 and 636 for WordNet, Wiktionary and VerbNet, respectively.", "We used WordNet as our core resource, exploiting BabelNet\u2019s mappings (Navigli and Ponzetto, 2012) as a bridge between Wiktionary and VerbNet to WordNet."], "citedPaper": {"paperId": "bb6898d6041e97c4946661b3a3df0f82286a43b5", "externalIds": {"MAG": "1579035156", "CorpusId": 60771008}, "url": "https://www.semanticscholar.org/paper/bb6898d6041e97c4946661b3a3df0f82286a43b5", "title": "Verbnet: a broad-coverage, comprehensive verb lexicon", "abstract": "Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. \nIn order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. \nOne of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment. In order to support the animation of the actions, PARs have to make explicit many details that are often underspecified in the language. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. \nTo evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank. VerbNet syntactic frames account for over 84% exact matches to the frames found in PropBank. \nVerbNet provides mappings between its verbs and WordNet senses and between its verbs and FameNet II frames, and mappings between the syntactic frames and Xtag tree families. All these resources are complementary and can be used as extensions of each other. \nThe original set of classes described by Levin has been refined and extended in many ways through systematic efforts: the coverage experiment against PropBank corpus instances proposed a large set of new syntactic frames and a better treatment of prepositions; new classes from Korhonen and Briscoe's resource were integrated into the lexicon; and new members from the LCS database were added. \nTaking advantage of VerbNet's class-based approach automatic acquisition methods were investigated. Additional verbs derived from Kingsbury's clustering experiments and from Loper's VerbNet-WordNet correlation experiment were integrated into the lexicon. These experiments show that it is possible to semi-automatically supplement and tune VerbNet with novel information from corpus data. These approaches reduce the manual classification and enable easy adaptation of the lexicon to specific tasks and applications.", "venue": "", "year": 2005, "citationCount": 1077, "influentialCitationCount": 156, "authors": [{"authorId": "2135004", "name": "K. Schuler"}, {"authorId": "145755155", "name": "Martha Palmer"}]}}, {"contexts": [], "citedPaper": {"paperId": null, "externalIds": null, "url": null, "title": "WordNet: An Electronic Database", "abstract": null, "venue": "", "year": 1998, "citationCount": null, "influentialCitationCount": null, "authors": []}}, {"contexts": ["\u2026for individual meanings of words, and\n1Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949).\ncontextualized word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given\u2026", "Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949)."], "citedPaper": {"paperId": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac", "externalIds": {"MAG": "208069425", "DOI": "10.1037/h0052803", "CorpusId": 141120597}, "url": "https://www.semanticscholar.org/paper/2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac", "title": "Human behavior and the principle of least effort", "abstract": null, "venue": "", "year": 1949, "citationCount": 7399, "influentialCitationCount": 421, "authors": [{"authorId": "4632093", "name": "G. Zipf"}]}}], "next": null}}