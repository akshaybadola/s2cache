{"abstract": "This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.", "arxivId": "2106.10165", "authors": [{"authorId": "152158598", "name": "Daniel A. Roberts", "url": "https://www.semanticscholar.org/author/152158598"}, {"authorId": "8904571", "name": "Sho Yaida", "url": "https://www.semanticscholar.org/author/8904571"}, {"authorId": "30030844", "name": "B. Hanin", "url": "https://www.semanticscholar.org/author/30030844"}], "citationVelocity": 0, "citations": [{"arxivId": "2107.01562", "authors": [{"authorId": "30030844", "name": "B. Hanin"}], "doi": null, "intent": ["background"], "isInfluential": false, "paperId": "2614af578a3ad726d33af80c49c69d2d5700f3b3", "title": "Random Neural Networks in the Infinite Width Limit as Gaussian Processes", "url": "https://www.semanticscholar.org/paper/2614af578a3ad726d33af80c49c69d2d5700f3b3", "venue": "ArXiv", "year": 2021}, {"arxivId": "2105.14625", "authors": [{"authorId": "1393588417", "name": "T. Bartz-Beielstein"}], "doi": null, "intent": [], "isInfluential": false, "paperId": "346f424dcdc137175ecadae949a5f9cc1714f1be", "title": "Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT", "url": "https://www.semanticscholar.org/paper/346f424dcdc137175ecadae949a5f9cc1714f1be", "venue": "ArXiv", "year": 2021}, {"arxivId": "2107.06898", "authors": [{"authorId": "5220586", "name": "J. Erdmenger"}, {"authorId": "11015879", "name": "Kevin T. Grosvenor"}, {"authorId": "46210148", "name": "Robert A. Jefferson"}], "doi": null, "intent": ["background"], "isInfluential": false, "paperId": "9908e7b8947f84b4f18b59d9c1c4d86518c50ce1", "title": "Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group", "url": "https://www.semanticscholar.org/paper/9908e7b8947f84b4f18b59d9c1c4d86518c50ce1", "venue": "ArXiv", "year": 2021}, {"arxivId": "2108.01403", "authors": [{"authorId": "51265836", "name": "H. Erbin"}, {"authorId": "88741933", "name": "Vincent Lahoche"}, {"authorId": "88740604", "name": "D. O. Samary"}], "doi": null, "intent": [], "isInfluential": false, "paperId": "d70511534eefaef6baa3abfaa3415e28ea20c377", "title": "Nonperturbative renormalization for the neural network-QFT correspondence", "url": "https://www.semanticscholar.org/paper/d70511534eefaef6baa3abfaa3415e28ea20c377", "venue": "", "year": 2021}], "corpusId": 235485320, "doi": null, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "influentialCitationCount": 0, "isOpenAccess": false, "isPublisherLicensed": true, "is_open_access": false, "is_publisher_licensed": true, "numCitedBy": 4, "numCiting": 0, "paperId": "5b5535418882e9543a33819592c5bf371e68b2c3", "references": [], "title": "The Principles of Deep Learning Theory", "topics": [], "url": "https://www.semanticscholar.org/paper/5b5535418882e9543a33819592c5bf371e68b2c3", "venue": "ArXiv", "year": 2021}